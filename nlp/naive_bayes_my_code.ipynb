{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'positive': {'i': 1, 'love': 1, 'coding': 1, 'python': 1, 'is': 1, 'amazing': 1}, 'negative': {'i': 1, 'hate': 1, 'bugs': 1, 'debugging': 1, 'is': 1, 'hard': 1}}\n",
      "positive\n",
      "negative\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import math\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "class NaiveBayesTextClassifier:\n",
    "    def __init__(self):\n",
    "        self.vocab = set()  # Unique words in training data\n",
    "        self.class_counts = Counter()  # Number of documents per class\n",
    "        self.word_counts = {}  # Word frequency per class\n",
    "        self.total_docs = 0  # Total documents in training data\n",
    "\n",
    "    def preprocess(self, text):\n",
    "        \"\"\"Lowercase & remove non-alphabetic characters\"\"\"\n",
    "        text = text.lower()\n",
    "        words = re.findall(r'\\b[a-z]+\\b', text)  # Extract words and put into list\n",
    "        return words\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        \"\"\"Train the model using word frequencies\"\"\"\n",
    "        self.total_docs = len(y_train)\n",
    "        self.class_counts = Counter(y_train)\n",
    "        for x, y in zip(X_train, y_train):\n",
    "            x = self.preprocess(x)\n",
    "            self.vocab.update(x)\n",
    "            for word in x:\n",
    "                if y not in self.word_counts:\n",
    "                    self.word_counts[y]={}\n",
    "                if word not in self.word_counts[y]:\n",
    "                    self.word_counts[y][word]=1\n",
    "                else:\n",
    "                    self.word_counts[y][word]+=1\n",
    "        print(self.word_counts)\n",
    "\n",
    "    def predict(self, text):\n",
    "        \"\"\"Predict the class of a new document\"\"\"\n",
    "        words = self.preprocess(text)\n",
    "        class_probs = {}  # Store log probabilities for each class\n",
    "\n",
    "        for label in self.class_counts.keys(): #Iterate Over Each Class and Compute Probabilities\n",
    "            # Compute log prior probability P(y)\n",
    "            # P(y) = (# of documents in class y) / (total documents)\n",
    "            prior = math.log(self.class_counts[label] / self.total_docs)\n",
    "\n",
    "            # Compute log likelihood P(x|y) with Laplace smoothing\n",
    "            #total_words_in_class: Total word occurrences in a class.\n",
    "            #vocab_size: Number of unique words in training data.\n",
    "            total_words_in_class = sum(self.word_counts[label].values())\n",
    "            vocab_size = len(self.vocab)\n",
    "\n",
    "            #Laplace Smoothing (+1) ensures no word has zero probability.\n",
    "            for word in words:\n",
    "                word_freq = self.word_counts[label].get(word, 0) + 1  # Laplace smoothing\n",
    "                word_prob = word_freq / (total_words_in_class + vocab_size)\n",
    "                prior += math.log(word_prob)  # Sum log probabilities\n",
    "\n",
    "            class_probs[label] = prior\n",
    "\n",
    "        return max(class_probs, key=class_probs.get)  # Return the class with the highest probability\n",
    "\n",
    "# === Example Usage ===\n",
    "X_train = [\n",
    "    \"I love coding\",\n",
    "    \"Python is amazing\",\n",
    "    \"I hate bugs\",\n",
    "    \"Debugging is hard\"\n",
    "]\n",
    "\n",
    "y_train = [\"positive\", \"positive\", \"negative\", \"negative\"]\n",
    "\n",
    "# Train the model\n",
    "nb_classifier = NaiveBayesTextClassifier()\n",
    "nb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict new examples\n",
    "print(nb_classifier.predict(\"I enjoy coding in Python\"))  # Likely: \"positive\"\n",
    "print(nb_classifier.predict(\"I hate bugs its hard to fix\"))  # Likely: \"negative\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tech', 'health', 'finance']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_probs = {\"tech\":1, \"finance\": 1.2, \"health\": 1.3}\n",
    "sorted_dict = sorted(class_probs, reverse= True)\n",
    "sorted_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [\n",
    "    \"I love coding\",\n",
    "    \"Python is amazing\",\n",
    "    \"I hate bugs\",\n",
    "    \"Debugging is hard\"\n",
    "]\n",
    "\n",
    "y_train = [\"positive\", \"positive\", \"negative\", \"negative\"]\n",
    "\n",
    "# After calling fit(), the model stores:\n",
    "#Class counts (how many examples per class)\n",
    "{'positive': 2, 'negative': 2}\n",
    "\n",
    "#\t2.\tWord counts per class\n",
    "# self.word_counts\n",
    "{\n",
    "    'positive': {'I': 1, 'love': 1, 'coding': 1, 'Python': 1, 'is': 1, 'amazing': 1},\n",
    "    'negative': {'I': 1, 'hate': 1, 'bugs': 1, 'Debugging': 1, 'is': 1, 'hard': 1}\n",
    "}\n",
    "#\t3.\tVocabulary (unique words across all texts)\n",
    "{'I', 'love', 'coding', 'Python', 'is', 'amazing', 'hate', 'bugs', 'Debugging', 'hard'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.word_counts['positive'] = {'i': 1, 'love': 1, 'coding': 1, 'python': 1, 'is': 1, 'amazing': 1}\n",
    "total_words_in_class = 6\n",
    "vocab_size = 10\n",
    "\n",
    "#\tFor \"love\":\n",
    "word_freq = 1 + 1 = 2\n",
    "word_prob = 2 / (6 + 10) = 2/16 = 0.125\n",
    "log_prob += log(0.125) = -2.08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' likelihood 是怎么算的\n",
    "P(word | class) = \\frac{\\text{word count in class} + 1}{\\text{total words in class} + \\text{vocabulary size}}\n",
    "\n",
    "If we calculate probabilities without smoothing, the probability of a word given a class is:\n",
    "\n",
    "这个容易理解；\n",
    "P(word | class) = \\frac{\\text{word count in class}}{\\text{total words in class}}\n",
    "\n",
    "\n",
    "However, if a word never appeared in the training data for a particular class, its count is zero, which results in:\n",
    "\n",
    "\n",
    "P(word | class) = \\frac{0}{\\text{total words in class}} = 0\n",
    "\n",
    "\n",
    "Since Naive Bayes multiplies probabilities, having even one zero probability makes the entire class probability zero. This can wrongly eliminate the class from consideration.\n",
    "\n",
    "Laplace Smoothing fixes this by adding 1 to each word count.\n",
    "\n",
    "\n",
    "Numerator:  \\text{word count in class} + 1\n",
    "\t•\tIf a word appears  N  times in a class, we count it as  N+1 .\n",
    "\t•\tIf a word never appeared ( N = 0 ), we still give it a small probability ( 0 + 1 = 1 ).\n",
    "\t•\tThis prevents zero probabilities.\n",
    "\n",
    "Denominator:  \\text{total words in class} + \\text{vocabulary size}\n",
    "\t•\tThe total count of all words in the class is increased by the vocabulary size (the number of unique words in all training data).\n",
    "\t•\tThis ensures that the probabilities remain valid and still sum to 1.\n",
    "\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
