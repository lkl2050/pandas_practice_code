{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73132d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentiment_minimal.py\n",
    "import torch, torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from collections import Counter\n",
    "\n",
    "# 1) tiny toy data\n",
    "train_texts = [\"i love this movie\", \"this film is great\", \"i hate this movie\", \"this film is bad\", \"awesome acting\", \"terrible plot\"]\n",
    "train_labels = [1,1,0,0,1,0]\n",
    "test_texts = [\"great movie\", \"bad acting\", \"i love it\", \"i hate it\"]\n",
    "test_labels = [1,0,1,0]\n",
    "\n",
    "# 2) tokenizer\n",
    "def tok(s): return s.lower().split()\n",
    "\n",
    "# 3) vocab from train only\n",
    "PAD, UNK = \"<pad>\", \"<unk>\"\n",
    "cnt = Counter(w for s in train_texts for w in tok(s))\n",
    "itos = [PAD, UNK] + [w for w,_ in cnt.items()]\n",
    "stoi = {w:i for i,w in enumerate(itos)}\n",
    "pad_id, unk_id = stoi[PAD], stoi[UNK]\n",
    "\n",
    "# 4) numericalize\n",
    "def to_ids(s): return torch.tensor([stoi.get(w, unk_id) for w in tok(s)], dtype=torch.long)\n",
    "\n",
    "# 5) datasets\n",
    "train_X = [to_ids(s) for s in train_texts]\n",
    "train_y = torch.tensor(train_labels, dtype=torch.long)\n",
    "test_X = [to_ids(s) for s in test_texts]\n",
    "test_y = torch.tensor(test_labels, dtype=torch.long)\n",
    "\n",
    "# 6) collate: pad + mask (mask optional here)\n",
    "def collate(batch):\n",
    "    seqs, labels = zip(*batch)\n",
    "    lens = torch.tensor([len(s) for s in seqs])\n",
    "    padded = pad_sequence(seqs, batch_first=True, padding_value=pad_id)\n",
    "    return padded, lens, torch.tensor(labels)\n",
    "\n",
    "# 7) dataloaders\n",
    "train_ds = list(zip(train_X, train_y.tolist()))\n",
    "test_ds = list(zip(test_X, test_y.tolist()))\n",
    "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=2, shuffle=True, collate_fn=collate)\n",
    "test_dl  = torch.utils.data.DataLoader(test_ds,  batch_size=2, shuffle=False, collate_fn=collate)\n",
    "\n",
    "\n",
    "\n",
    "# # pad once (batch_first=True → shape (N, T))\n",
    "# pad_id = stoi[\"<pad>\"]\n",
    "# train_X_pad = pad_sequence(train_X, batch_first=True, padding_value=pad_id)\n",
    "# test_X_pad  = pad_sequence(test_X,  batch_first=True, padding_value=pad_id)\n",
    "\n",
    "# # lengths and masks (optional)\n",
    "# train_lens = torch.tensor([len(s) for s in train_X])\n",
    "# test_lens  = torch.tensor([len(s) for s in test_X])\n",
    "# train_mask = (train_X_pad != pad_id)  # (N, T) bool\n",
    "# test_mask  = (test_X_pad  != pad_id)\n",
    "\n",
    "# # use default collate (no collate_fn)\n",
    "# train_ds = TensorDataset(train_X_pad, train_lens, train_mask, train_y)\n",
    "# test_ds  = TensorDataset(test_X_pad,  test_lens,  test_mask,  test_y)\n",
    "# train_dl = DataLoader(train_ds, batch_size=2, shuffle=True)\n",
    "# test_dl  = DataLoader(test_ds,  batch_size=2, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "# 8) model: Embedding → mean → Linear\n",
    "class MeanEmbClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim=64, num_classes=2, pad_idx=0):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_idx)\n",
    "        self.fc = nn.Linear(emb_dim, num_classes)\n",
    "    def forward(self, x, lens):\n",
    "        e = self.emb(x)                      # (B,T,E)\n",
    "        e = e.sum(dim=1) / lens.unsqueeze(1) # mean over tokens (ignore pads via lengths)\n",
    "        return self.fc(e)\n",
    "    \n",
    "\n",
    "#     \t•\tdim=1 means sum across the sequence length T.\n",
    "# \t•\tSo it adds up all token embeddings in a sentence.\n",
    "# \t•\tShape becomes (B, E)\n",
    "\n",
    "# Example: if T=10, we collapse it into just one 64-d vector per sentence (sum of 10 embeddings).\n",
    "\n",
    "# ⸻\n",
    "\n",
    "# 3. lens.unsqueeze(1)\n",
    "# \t•\tlens is the tensor of sequence lengths (how many real tokens per sample, not counting <pad>).\n",
    "# Example: lens = tensor([7, 5, 9])\n",
    "# \t•\tShape of lens: (B,)\n",
    "# \t•\t.unsqueeze(1) → (B, 1) so it can broadcast in division.\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = MeanEmbClassifier(len(itos), emb_dim=64, num_classes=2, pad_idx=pad_id).to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# 9) train\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    tot, correct, total = 0.0, 0, 0\n",
    "    for xb, lens, yb in train_dl:\n",
    "        xb,lens,yb = xb.to(device), lens.to(device), yb.to(device)\n",
    "        logits = model(xb, lens)\n",
    "        loss = loss_fn(logits, yb)\n",
    "        opt.zero_grad(); loss.backward(); opt.step()\n",
    "        tot += loss.item()*xb.size(0)\n",
    "        pred = logits.argmax(dim=1)\n",
    "        correct += (pred==yb).sum().item()\n",
    "        total += xb.size(0)\n",
    "    print(f\"epoch {epoch+1}: loss={tot/total:.4f}, acc={correct/total:.3f}\")\n",
    "\n",
    "# 10) eval\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct, total = 0, 0\n",
    "    for xb, lens, yb in test_dl:\n",
    "        xb,lens,yb = xb.to(device), lens.to(device), yb.to(device)\n",
    "        pred = model(xb, lens).argmax(dim=1)\n",
    "        correct += (pred==yb).sum().item()\n",
    "        total += xb.size(0)\n",
    "    print(f\"test acc: {correct/total:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
