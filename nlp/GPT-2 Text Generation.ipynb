{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a Simplified GPT-2-like Text Generation Function\n",
    "You are tasked with implementing a simplified GPT-2-like text generation function in Python. This function will incorporate the following components of a minimal GPT-2 architecture:\n",
    "\n",
    "Token Embeddings: Map input tokens to dense vector representations.\n",
    "Positional Embeddings: Add positional information to token embeddings.\n",
    "Multi-head Attention: Attend to various parts of the sequence.\n",
    "Feed-Forward Network: Process attention outputs through a dense layer.\n",
    "Layer Normalization: Stabilize the training process.\n",
    "The function must take in the following parameters:\n",
    "\n",
    "Prompt: The initial text to guide the generation process.\n",
    "Number of Tokens to Generate: Specify how many tokens to output.\n",
    "Your function should output the generated text.\n",
    "\n",
    "Additionally, utilize the helper function load_encoder_hparams_and_params to retrieve:\n",
    "\n",
    "A dummy encoder.\n",
    "Model hyperparameters.\n",
    "Model parameters.\n",
    "Build your text generation logic around these components. This exercise is designed to help you understand the core concepts behind GPT-2's autoregressive text generation.\n",
    "\n",
    "Example:\n",
    "Input:\n",
    "prompt=\"hello\", n_tokens_to_generate=5\n",
    "Output:\n",
    "world <UNK> <UNK> <UNK> <UNK>\n",
    "Reasoning:\n",
    "The function encodes the input \"hello\" into tokens using the dummy encoder, then runs a simplified GPT-2 forward pass to generate 5 tokens. Finally, it decodes the generated tokens back into text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def gelu(x):\n",
    "    return 0.5 * x * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * x**3)))\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "\n",
    "def layer_norm(x, g, b, eps=1e-5):\n",
    "    mean = np.mean(x, axis=-1, keepdims=True)\n",
    "    variance = np.var(x, axis=-1, keepdims=True)\n",
    "    return g * (x - mean) / np.sqrt(variance + eps) + b\n",
    "\n",
    "def linear(x, w, b):\n",
    "    return x @ w + b\n",
    "\n",
    "def ffn(x, c_fc, c_proj):\n",
    "    return linear(gelu(linear(x, **c_fc)), **c_proj)\n",
    "\n",
    "def attention(q, k, v, mask):\n",
    "    return softmax(q @ k.T / np.sqrt(q.shape[-1]) + mask) @ v\n",
    "\n",
    "def mha(x, c_attn, c_proj, n_head):\n",
    "    x = linear(x, **c_attn)\n",
    "    qkv_heads = list(map(lambda x: np.split(x, n_head, axis=-1), np.split(x, 3, axis=-1)))\n",
    "    causal_mask = (1 - np.tri(x.shape[0], dtype=x.dtype)) * -1e10\n",
    "    out_heads = [attention(q, k, v, causal_mask) for q, k, v in zip(*qkv_heads)]\n",
    "    x = linear(np.hstack(out_heads), **c_proj)\n",
    "    return x\n",
    "\n",
    "def transformer_block(x, mlp, attn, ln_1, ln_2, n_head):\n",
    "    x = x + mha(layer_norm(x, **ln_1), **attn, n_head=n_head)\n",
    "    x = x + ffn(layer_norm(x, **ln_2), **mlp)\n",
    "    return x\n",
    "\n",
    "def gpt2(inputs, wte, wpe, blocks, ln_f, n_head):\n",
    "    x = wte[inputs] + wpe[range(len(inputs))]\n",
    "    for block in blocks:\n",
    "        x = transformer_block(x, **block, n_head=n_head)\n",
    "    return layer_norm(x, **ln_f) @ wte.T\n",
    "\n",
    "def generate(inputs, params, n_head, n_tokens_to_generate):\n",
    "    for _ in range(n_tokens_to_generate):\n",
    "        logits = gpt2(inputs, **params, n_head=n_head)\n",
    "        next_id = np.argmax(logits[-1])\n",
    "        inputs.append(int(next_id))\n",
    "    return inputs[len(inputs) - n_tokens_to_generate:]\n",
    "\n",
    "def gen_text(prompt: str, n_tokens_to_generate: int = 40):\n",
    "    np.random.seed(42)  # Set the random seed for reproducibility\n",
    "    encoder, hparams, params = load_encoder_hparams_and_params()\n",
    "    input_ids = encoder.encode(prompt)\n",
    "    assert len(input_ids) + n_tokens_to_generate < hparams[\"n_ctx\"]\n",
    "    output_ids = generate(input_ids, params, hparams[\"n_head\"], n_tokens_to_generate)\n",
    "    output_text = encoder.decode(output_ids)\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy encoder\n",
    "class DummyEncoder:\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "        self.token_to_id = {tok:i for i,tok in enumerate(vocab)}\n",
    "        self.id_to_token = {i:t for i,t in enumerate(vocab)}\n",
    "    def encode(self, text):\n",
    "        return [self.token_to_id.get(tok, self.token_to_id[\"<UNK>\"]) for tok in text.split()]\n",
    "    def decode(self, ids):\n",
    "        return \" \".join(self.id_to_token.get(i,\"<UNK>\") for i in ids)\n",
    "\n",
    "def load_encoder_hparams_and_params():\n",
    "    vocab = [\"hello\",\"world\",\"cat\",\"dog\",\"<UNK>\"]\n",
    "    encoder = DummyEncoder(vocab)\n",
    "\n",
    "    d_model = 8\n",
    "    n_head = 2\n",
    "    n_ctx = 20\n",
    "    V = len(vocab)\n",
    "\n",
    "    # Tiny random weights for demo\n",
    "    rng = np.random.default_rng(42)\n",
    "    wte = rng.normal(size=(V,d_model))\n",
    "    wpe = rng.normal(size=(n_ctx,d_model))\n",
    "    ln_f = {\"g\": np.ones(d_model), \"b\": np.zeros(d_model)}\n",
    "\n",
    "    # One block only, super minimal\n",
    "    block = {\n",
    "        \"mlp\": {\n",
    "            \"c_fc\": {\"w\": rng.normal(size=(d_model,d_model)), \"b\": np.zeros(d_model)},\n",
    "            \"c_proj\": {\"w\": rng.normal(size=(d_model,d_model)), \"b\": np.zeros(d_model)}\n",
    "        },\n",
    "        \"attn\": {\n",
    "            \"c_attn\": {\"w\": rng.normal(size=(d_model,3*d_model)), \"b\": np.zeros(3*d_model)},\n",
    "            \"c_proj\": {\"w\": rng.normal(size=(d_model,d_model)), \"b\": np.zeros(d_model)}\n",
    "        },\n",
    "        \"ln_1\": {\"g\": np.ones(d_model), \"b\": np.zeros(d_model)},\n",
    "        \"ln_2\": {\"g\": np.ones(d_model), \"b\": np.zeros(d_model)}\n",
    "    }\n",
    "\n",
    "    params = {\"wte\":wte,\"wpe\":wpe,\"blocks\":[block],\"ln_f\":ln_f}\n",
    "    hparams = {\"n_ctx\":n_ctx,\"n_head\":n_head}\n",
    "    return encoder, hparams, params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog <UNK> <UNK> cat dog\n",
      "<UNK> <UNK> cat dog <UNK>\n",
      "<UNK> <UNK> cat dog world\n"
     ]
    }
   ],
   "source": [
    "print(gen_text(\"hello\", 5))\n",
    "print(gen_text(\"cat\", 5))\n",
    "print(gen_text(\"dog\", 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ----- tiny helpers -----\n",
    "def gelu(x): return 0.5*x*(1+np.tanh(np.sqrt(2/np.pi)*(x+0.044715*x**3)))\n",
    "def softmax(x): x=x-np.max(x,axis=-1,keepdims=True); e=np.exp(x); return e/np.sum(e,axis=-1,keepdims=True)\n",
    "def layer_norm(x,g,b,eps=1e-5): m=x.mean(-1,keepdims=True); v=x.var(-1,keepdims=True); return g*(x-m)/np.sqrt(v+eps)+b\n",
    "def causal_mask(T): m=np.triu(np.ones((T,T)),1); return np.where(m==1,-1e9,0.0)\n",
    "\n",
    "# ----- dummy encoder -----\n",
    "class DummyEncoder:\n",
    "    def __init__(self,vocab): self.vocab=vocab; self.t2i={t:i for i,t in enumerate(vocab)}; self.i2t={i:t for i,t in enumerate(vocab)}\n",
    "    def encode(self,text): return [self.t2i.get(tok,self.t2i[\"<UNK>\"]) for tok in text.split()]\n",
    "    def decode(self,ids): return \" \".join(self.i2t.get(int(i),\"<UNK>\") for i in ids)\n",
    "\n",
    "# ----- tiny GPT block (single head) -----\n",
    "def linear(x,w,b): return x@w + b\n",
    "\n",
    "def attn(Q,K,V,mask): scores=(Q@K.T)/np.sqrt(Q.shape[-1]); scores+=mask; A=softmax(scores); return A@V\n",
    "\n",
    "\n",
    "def transformer_block(x,params):\n",
    "    # pre-norm + self-attn\n",
    "    y=layer_norm(x,params[\"ln1_g\"],params[\"ln1_b\"])\n",
    "    qkv=linear(y,params[\"attn_w\"],params[\"attn_b\"])             # (T,3D)\n",
    "    q,k,v=np.split(qkv,3,axis=-1)                                # (T,D) each\n",
    "    out=attn(q,k,v,params[\"mask\"])                               # (T,D)\n",
    "    x=x+linear(out,params[\"attn_proj_w\"],params[\"attn_proj_b\"])  # residual\n",
    "    # pre-norm + MLP\n",
    "    y=layer_norm(x,params[\"ln2_g\"],params[\"ln2_b\"])\n",
    "    h=gelu(linear(y,params[\"ff_w\"],params[\"ff_b\"]))              # expand->gelu\n",
    "    h=linear(h,params[\"ffp_w\"],params[\"ffp_b\"])                  # project back\n",
    "    return x+h                                                   # residual\n",
    "\n",
    "def logits_from_ids(ids,params):\n",
    "    T=len(ids); x=params[\"wte\"][ids]+params[\"wpe\"][:T]\n",
    "    params[\"mask\"]=causal_mask(T)\n",
    "    x=transformer_block(x,params)\n",
    "    x=layer_norm(x,params[\"lnf_g\"],params[\"lnf_b\"])\n",
    "    return x@params[\"wte\"].T                                     # weight tying\n",
    "\n",
    "def generate(prompt_ids,params,n_tokens):\n",
    "    ids=list(prompt_ids)\n",
    "    for _ in range(n_tokens):\n",
    "        lg=logits_from_ids(ids,params)\n",
    "        next_id=int(np.argmax(lg[-1]))                           # greedy\n",
    "        ids.append(next_id)\n",
    "    return ids[-n_tokens:]\n",
    "\n",
    "# ----- minimal wiring + sample -----\n",
    "def build_tiny_gpt(vocab_size=8,d_model=16,ff_mult=4,n_ctx=64,seed=0):\n",
    "    rng=np.random.default_rng(seed); V=vocab_size; D=d_model; F=D*ff_mult\n",
    "    wte=rng.normal(0,0.02,(V,D)); wpe=rng.normal(0,0.02,(n_ctx,D))\n",
    "    attn_w=rng.normal(0,0.02,(D,3*D)); attn_b=np.zeros(3*D)\n",
    "    attn_proj_w=rng.normal(0,0.02,(D,D)); attn_proj_b=np.zeros(D)\n",
    "    ff_w=rng.normal(0,0.02,(D,F)); ff_b=np.zeros(F)\n",
    "    ffp_w=rng.normal(0,0.02,(F,D)); ffp_b=np.zeros(D)\n",
    "    ln1_g=np.ones(D); ln1_b=np.zeros(D)\n",
    "    ln2_g=np.ones(D); ln2_b=np.zeros(D)\n",
    "    lnf_g=np.ones(D); lnf_b=np.zeros(D)\n",
    "    return {\"wte\":wte,\"wpe\":wpe,\"attn_w\":attn_w,\"attn_b\":attn_b,\"attn_proj_w\":attn_proj_w,\"attn_proj_b\":attn_proj_b,\"ff_w\":ff_w,\"ff_b\":ff_b,\"ffp_w\":ffp_w,\"ffp_b\":ffp_b,\"ln1_g\":ln1_g,\"ln1_b\":ln1_b,\"ln2_g\":ln2_g,\"ln2_b\":ln2_b,\"lnf_g\":lnf_g,\"lnf_b\":lnf_b}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # tiny vocab; space-separated tokens only\n",
    "    vocab=[\"hello\",\"world\",\"cat\",\"dog\",\"a\",\"b\",\"c\",\"<UNK>\"]\n",
    "    enc=DummyEncoder(vocab)\n",
    "    params=build_tiny_gpt(vocab_size=len(vocab),d_model=16,ff_mult=4,n_ctx=64,seed=42)\n",
    "    # run a few samples\n",
    "    for prompt in [\"hello\",\"cat dog\",\"hello world\"]:\n",
    "        inp=enc.encode(prompt)\n",
    "        out_ids=generate(inp,params,n_tokens=5)\n",
    "        print(f\"prompt='{prompt}' â†’\", enc.decode(out_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, torch, torch.nn as nn, torch.nn.functional as F\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, n_head: int, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        assert d_model % n_head == 0\n",
    "        self.n_head = n_head\n",
    "        self.d_head = d_model // n_head\n",
    "        self.qkv = nn.Linear(d_model, 3*d_model, bias=False)\n",
    "        self.proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.attn_drop = nn.Dropout(dropout)\n",
    "        self.resid_drop = nn.Dropout(dropout)\n",
    "        self.register_buffer(\"mask\", None, persistent=False)\n",
    "    def _get_mask(self, T: int, device):\n",
    "        if self.mask is None or self.mask.size(0) < T:\n",
    "            m = torch.tril(torch.ones(T, T, dtype=torch.bool, device=device))\n",
    "            self.mask = m\n",
    "        return self.mask[:T, :T]\n",
    "    def forward(self, x):  # x: (B,T,D)\n",
    "        B,T,D = x.size()\n",
    "        qkv = self.qkv(x)                         # (B,T,3D)\n",
    "        q,k,v = qkv.split(D, dim=-1)              # each (B,T,D)\n",
    "        q = q.view(B,T,self.n_head,self.d_head).transpose(1,2)  # (B,H,T,dh)\n",
    "        k = k.view(B,T,self.n_head,self.d_head).transpose(1,2)\n",
    "        v = v.view(B,T,self.n_head,self.d_head).transpose(1,2)\n",
    "        att = (q @ k.transpose(-2,-1)) / math.sqrt(self.d_head) # (B,H,T,T)\n",
    "        att = att.masked_fill(~self._get_mask(T, x.device), float(\"-inf\"))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.attn_drop(att)\n",
    "        y = att @ v                                 # (B,H,T,dh)\n",
    "        y = y.transpose(1,2).contiguous().view(B,T,D)\n",
    "        y = self.resid_drop(self.proj(y))\n",
    "        return y\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, d_model: int, mult: int = 4, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(d_model, mult*d_model)\n",
    "        self.proj = nn.Linear(mult*d_model, d_model)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.drop(self.proj(x))\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, d_model: int, n_head: int, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.attn = CausalSelfAttention(d_model, n_head, dropout)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.mlp = MLP(d_model, mult=4, dropout=dropout)\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, vocab_size: int, n_ctx: int, d_model: int = 128, n_head: int = 4, n_layer: int = 2, dropout: float = 0.0, tie_weights: bool = True):\n",
    "        super().__init__()\n",
    "        self.n_ctx = n_ctx\n",
    "        self.tok_emb = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_emb = nn.Embedding(n_ctx, d_model)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.blocks = nn.ModuleList([Block(d_model, n_head, dropout) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(d_model)\n",
    "        self.head = nn.Linear(d_model, vocab_size, bias=False)\n",
    "        if tie_weights:\n",
    "            self.head.weight = self.tok_emb.weight\n",
    "        self.apply(self._init_weights)\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
    "            if m.bias is not None: nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, nn.Embedding):\n",
    "            nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
    "    def forward(self, idx):  # idx: (B,T) int64\n",
    "        B,T = idx.size()\n",
    "        assert T <= self.n_ctx\n",
    "        pos = torch.arange(0, T, device=idx.device).unsqueeze(0)  # (1,T)\n",
    "        x = self.tok_emb(idx) + self.pos_emb(pos)\n",
    "        x = self.drop(x)\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x)  # (B,T,V)\n",
    "        return logits\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, n_new_tokens: int, temperature: float = 1.0, top_k: int | None = None):\n",
    "        self.eval()\n",
    "        for _ in range(n_new_tokens):\n",
    "            idx_cond = idx[:, -self.n_ctx:]  # crop to context\n",
    "            logits = self(idx_cond)[:, -1, :] / max(temperature, 1e-6)\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, k=min(top_k, logits.size(-1)))\n",
    "                thresh = v[:, -1].unsqueeze(-1)\n",
    "                logits = torch.where(logits < thresh, torch.full_like(logits, float(\"-inf\")), logits)\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_id = torch.multinomial(probs, num_samples=1)     # sampling; for greedy: torch.argmax(logits, dim=-1, keepdim=True)\n",
    "            idx = torch.cat([idx, next_id], dim=1)\n",
    "        return idx\n",
    "\n",
    "# tiny dummy tokenizer so you can run a sample\n",
    "class DummyTokenizer:\n",
    "    def __init__(self, vocab): self.vocab=vocab; self.stoi={t:i for i,t in enumerate(vocab)}; self.itos={i:t for i,t in enumerate(vocab)}\n",
    "    def encode(self, s): return torch.tensor([[self.stoi.get(t, self.stoi[\"<unk>\"]) for t in s.split()]], dtype=torch.long)\n",
    "    def decode(self, ids): \n",
    "        if isinstance(ids, torch.Tensor): ids = ids.tolist()\n",
    "        if ids and isinstance(ids[0], list): ids = ids[0]\n",
    "        return \" \".join(self.itos.get(i, \"<unk>\") for i in ids)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    torch.manual_seed(0)\n",
    "    vocab = [\"hello\",\"world\",\"cat\",\"dog\",\"a\",\"b\",\"c\",\"<unk>\"]\n",
    "    tok = DummyTokenizer(vocab)\n",
    "    V = len(vocab); n_ctx = 32\n",
    "    model = GPT(vocab_size=V, n_ctx=n_ctx, d_model=128, n_head=4, n_layer=2, dropout=0.0)\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model.to(device)\n",
    "    # prompt and generate\n",
    "    prompt = \"hello\"\n",
    "    idx = tok.encode(prompt).to(device)        # (1,T)\n",
    "    out_idx = model.generate(idx, n_new_tokens=8, temperature=1.0, top_k=5)\n",
    "    print(\"prompt:\", prompt)\n",
    "    print(\"gen:\", tok.decode(out_idx[0, idx.size(1):].cpu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, torch, torch.nn as nn, torch.nn.functional as F\n",
    "\n",
    "# dummy whitespace tokenizer\n",
    "class Tok:\n",
    "    def __init__(self, vocab): self.v=vocab; self.s2i={t:i for i,t in enumerate(vocab)}; self.i2s={i:t for i,t in enumerate(vocab)}\n",
    "    def encode(self, s): return torch.tensor([[self.s2i.get(t, self.s2i[\"<unk>\"]) for t in s.split()]], dtype=torch.long)\n",
    "    def decode(self, ids): \n",
    "        if isinstance(ids, torch.Tensor): ids = ids.tolist()\n",
    "        if ids and isinstance(ids[0], list): ids = ids[0]\n",
    "        return \" \".join(self.i2s.get(i,\"<unk>\") for i in ids)\n",
    "\n",
    "class SimpleGPT(nn.Module):\n",
    "    def __init__(self, vocab_size, n_ctx=32, d=64):\n",
    "        super().__init__()\n",
    "        self.n_ctx, self.d = n_ctx, d\n",
    "        self.tok = nn.Embedding(vocab_size, d)\n",
    "        self.pos = nn.Embedding(n_ctx, d)\n",
    "        self.qkv = nn.Linear(d, 3*d, bias=False)      # single head\n",
    "        self.proj = nn.Linear(d, d, bias=False)\n",
    "        self.ln1 = nn.LayerNorm(d)\n",
    "        self.ff1 = nn.Linear(d, 4*d)\n",
    "        self.ff2 = nn.Linear(4*d, d)\n",
    "        self.ln2 = nn.LayerNorm(d)\n",
    "        self.head = nn.Linear(d, vocab_size, bias=False)\n",
    "        self.head.weight = self.tok.weight            # tie weights\n",
    "        self.register_buffer(\"mask\", torch.tril(torch.ones(n_ctx, n_ctx)).bool(), persistent=False)\n",
    "        self.apply(self._init)\n",
    "    def _init(self, m):\n",
    "        if isinstance(m, (nn.Linear, nn.Embedding)):\n",
    "            nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
    "            if getattr(m, \"bias\", None) is not None: nn.init.zeros_(m.bias)\n",
    "    def forward(self, idx):                           # idx: (B,T)\n",
    "        B,T = idx.shape\n",
    "        pos = torch.arange(T, device=idx.device).unsqueeze(0)\n",
    "        x = self.tok(idx) + self.pos(pos)            # (B,T,d)\n",
    "        # pre-norm + self-attn (single head)\n",
    "        y = self.ln1(x)\n",
    "        q,k,v = self.qkv(y).chunk(3, dim=-1)         # (B,T,d) each\n",
    "        att = (q @ k.transpose(-2,-1)) / math.sqrt(self.d)\n",
    "        att = att.masked_fill(~self.mask[:T, :T], float(\"-inf\"))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        x = x + (att @ v) @ self.proj.weight.T       # residual; proj is linear with no bias\n",
    "        # pre-norm + MLP\n",
    "        y = self.ln2(x)\n",
    "        x = x + self.ff2(F.gelu(self.ff1(y)))        # residual\n",
    "        logits = self.head(x)                        # (B,T,V)\n",
    "        return logits\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, n_new):\n",
    "        self.eval()\n",
    "        for _ in range(n_new):\n",
    "            idx_cond = idx[:, -self.n_ctx:]\n",
    "            logits = self(idx_cond)[:, -1, :]\n",
    "            next_id = torch.argmax(logits, dim=-1, keepdim=True) # greedy\n",
    "            idx = torch.cat([idx, next_id], dim=1)\n",
    "        return idx\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    torch.manual_seed(0)\n",
    "    vocab = [\"hello\",\"world\",\"cat\",\"dog\",\"a\",\"b\",\"c\",\"<unk>\"]\n",
    "    tok = Tok(vocab)\n",
    "    model = SimpleGPT(vocab_size=len(vocab), n_ctx=32, d=64)\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model.to(device)\n",
    "    for prompt in [\"hello\", \"cat dog\", \"hello world\"]:\n",
    "        x = tok.encode(prompt).to(device)\n",
    "        y = model.generate(x, n_new=6)[0, x.size(1):].cpu()\n",
    "        print(f\"{prompt} -> {tok.decode(y)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
