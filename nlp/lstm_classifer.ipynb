{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9651ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, math, random, argparse, json, os, numpy as np, torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc16e802",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "def get_device():\n",
    "    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "def save_ckpt(path, model, opt, epoch, best_metric):\n",
    "    torch.save({\"model\":model.state_dict(),\"opt\":opt.state_dict(),\"epoch\":epoch,\"best\":best_metric}, path)\n",
    "def load_ckpt(path, model, opt=None, map_location=\"cpu\"):\n",
    "    ckpt=torch.load(path, map_location=map_location); model.load_state_dict(ckpt[\"model\"])\n",
    "    if opt: opt.load_state_dict(ckpt[\"opt\"]); return ckpt.get(\"epoch\",0), ckpt.get(\"best\",None)\n",
    "    return ckpt.get(\"epoch\",0), ckpt.get(\"best\",None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bd18f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "_tok_re = re.compile(r\"[A-Za-z0-9']+\")\n",
    "def tokenize(text:str):\n",
    "    return _tok_re.findall(text.lower())\n",
    "def build_vocab(texts, min_freq=2, max_size=50000):\n",
    "    cnt=Counter()\n",
    "    for t in texts: cnt.update(tokenize(t))\n",
    "    specials=[\"<pad>\",\"<unk>\"]; vocab=specials[:]\n",
    "    for w,f in cnt.most_common():\n",
    "        if f<min_freq: break\n",
    "        if len(vocab)>=max_size: break\n",
    "        vocab.append(w)\n",
    "    stoi={w:i for i,w in enumerate(vocab)}; itos={i:w for w,i in stoi.items()}\n",
    "    return vocab, stoi, itos\n",
    "def numericalize(tokens, stoi, unk_idx=1):\n",
    "    return [stoi.get(tok, unk_idx) for tok in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf042aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, texts, labels, stoi, max_len=256):\n",
    "        self.texts=texts; self.labels=labels; self.stoi=stoi; self.max_len=max_len\n",
    "    def __len__(self): return len(self.texts)\n",
    "    def __getitem__(self, idx):\n",
    "        toks=tokenize(self.texts[idx])\n",
    "        ids=numericalize(toks, self.stoi)\n",
    "        if len(ids)>self.max_len: ids=ids[:self.max_len]\n",
    "        return torch.tensor(ids, dtype=torch.long), torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "def collate_pad(batch, pad_idx=0):\n",
    "    seqs, labels = zip(*batch)\n",
    "    lengths = torch.tensor([len(s) for s in seqs], dtype=torch.long)\n",
    "    max_len = max(lengths).item() if lengths.numel()>0 else 1\n",
    "    padded = torch.full((len(seqs), max_len), pad_idx, dtype=torch.long)\n",
    "    for i,s in enumerate(seqs): padded[i,:len(s)] = s\n",
    "    return padded, lengths, torch.stack(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448ad0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=200, hidden=256, num_layers=1, bidir=True, dropout=0.2, pad_idx=0):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden, num_layers=num_layers, batch_first=True, bidirectional=bidir, dropout=dropout if num_layers>1 else 0.0)\n",
    "        out_dim = hidden * (2 if bidir else 1)\n",
    "        self.fc = nn.Sequential(nn.Dropout(dropout), nn.Linear(out_dim, 2))\n",
    "    def forward(self, x, lengths):\n",
    "        emb = self.embedding(x)\n",
    "        packed = pack_padded_sequence(emb, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        out_packed, (h_n, c_n) = self.lstm(packed)\n",
    "        # Use last hidden states (concatenate directions)\n",
    "        if self.lstm.bidirectional:\n",
    "            h = torch.cat([h_n[-2], h_n[-1]], dim=1)\n",
    "        else:\n",
    "            h = h_n[-1]\n",
    "        logits = self.fc(h)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ad8b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(model, loader, opt, device, train=True, amp=True, clip=1.0):\n",
    "    crit = nn.CrossEntropyLoss()\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=(amp and train and device.type==\"cuda\"))\n",
    "    total_loss=0.0; all_y=[]; all_p=[]\n",
    "    if train: model.train()\n",
    "    else: model.eval()\n",
    "    for xb, lengths, yb in loader:\n",
    "        xb, lengths, yb = xb.to(device), lengths.to(device), yb.to(device)\n",
    "        if train: opt.zero_grad(set_to_none=True)\n",
    "        with torch.cuda.amp.autocast(enabled=(amp and device.type==\"cuda\")), torch.set_grad_enabled(train):\n",
    "            logits = model(xb, lengths)\n",
    "            loss = crit(logits, yb)\n",
    "        if train:\n",
    "            scaler.scale(loss).backward()\n",
    "            if clip: nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "            scaler.step(opt); scaler.update()\n",
    "        total_loss += loss.item()*xb.size(0)\n",
    "        preds = logits.argmax(1).detach().cpu().numpy()\n",
    "        all_p.extend(list(preds)); all_y.extend(list(yb.detach().cpu().numpy()))\n",
    "    avg_loss = total_loss/len(loader.dataset)\n",
    "    acc = accuracy_score(all_y, all_p)\n",
    "    f1 = f1_score(all_y, all_p)\n",
    "    return avg_loss, acc, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f888fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    set_seed(args.seed); device=get_device()\n",
    "    # TODO: Load IMDB data into lists of texts and labels below.\n",
    "    # texts_train, labels_train = [...]\n",
    "    # texts_val, labels_val = [...]\n",
    "    # For interview: you can simulate small samples or describe downloading via HuggingFace datasets.\n",
    "    # Example (pseudo):\n",
    "    # from datasets import load_dataset\n",
    "    # ds = load_dataset(\"imdb\")\n",
    "    # texts_train = [x[\"text\"] for x in ds[\"train\"]]; labels_train = [x[\"label\"] for x in ds[\"train\"]]\n",
    "    # texts_val = [x[\"text\"] for x in ds[\"test\"]]; labels_val = [x[\"label\"] for x in ds[\"test\"]]\n",
    "\n",
    "    raise_if_placeholder = False\n",
    "    if raise_if_placeholder:\n",
    "        raise RuntimeError(\"Replace the TODO with real IMDB loading code.\")\n",
    "\n",
    "    # For offline demo, hereâ€™s a tiny mock (replace in real run)\n",
    "    texts_train = [\"i love this movie it is great\",\"terrible film waste of time\",\"fantastic acting and plot\",\"bad script and worse ending\"]*256\n",
    "    labels_train = [1,0,1,0]*256\n",
    "    texts_val = [\"great movie\",\"worst movie ever\",\"it was okay not great\",\"absolutely loved it\"]*64\n",
    "    labels_val = [1,0,0,1]*64\n",
    "\n",
    "    vocab, stoi, itos = build_vocab(texts_train, min_freq=args.min_freq, max_size=args.vocab_size)\n",
    "    pad_idx=0; unk_idx=1\n",
    "    train_ds = IMDBDataset(texts_train, labels_train, stoi, max_len=args.max_len)\n",
    "    val_ds = IMDBDataset(texts_val, labels_val, stoi, max_len=args.max_len)\n",
    "    train_loader = DataLoader(train_ds, batch_size=args.batch_size, shuffle=True, num_workers=4, pin_memory=True, collate_fn=lambda b: collate_pad(b,pad_idx))\n",
    "    val_loader = DataLoader(val_ds, batch_size=args.batch_size, shuffle=False, num_workers=4, pin_memory=True, collate_fn=lambda b: collate_pad(b,pad_idx))\n",
    "\n",
    "    model = LSTMClassifier(vocab_size=len(vocab), embed_dim=args.embed_dim, hidden=args.hidden, num_layers=args.num_layers, bidir=not args.unidirectional, dropout=args.dropout, pad_idx=pad_idx).to(device)\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "\n",
    "    best_f1=-1.0; bad=0\n",
    "    for epoch in range(1, args.epochs+1):\n",
    "        tr_loss, tr_acc, tr_f1 = run_epoch(model, train_loader, opt, device, train=True, amp=True, clip=1.0)\n",
    "        va_loss, va_acc, va_f1 = run_epoch(model, val_loader, opt, device, train=False, amp=False)\n",
    "        print(f\"epoch={epoch} train_loss={tr_loss:.4f} acc={tr_acc:.4f} f1={tr_f1:.4f} val_loss={va_loss:.4f} acc={va_acc:.4f} f1={va_f1:.4f}\")\n",
    "        if va_f1>best_f1:\n",
    "            best_f1=va_f1; bad=0; save_ckpt(\"best_imdb_lstm.pt\", model, opt, epoch, best_f1)\n",
    "        else:\n",
    "            bad+=1\n",
    "            if bad>=args.patience:\n",
    "                print(\"early stopping\"); break\n",
    "\n",
    "    # Load best and report\n",
    "    load_ckpt(\"best_imdb_lstm.pt\", model, None, map_location=device)\n",
    "    va_loss, va_acc, va_f1 = run_epoch(model, val_loader, opt, device, train=False, amp=False)\n",
    "    print(f\"best_eval val_loss={va_loss:.4f} val_acc={va_acc:.4f} val_f1={va_f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
