{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tech\n",
      "pets\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "class NaiveBayesTextClassifier:\n",
    "    def __init__(self):\n",
    "        self.vocab = set()  # Unique words in training data\n",
    "        self.class_counts = defaultdict(int)  # Number of documents per class\n",
    "        self.word_counts = defaultdict(lambda: defaultdict(int))  # Word frequency per class\n",
    "        self.total_docs = 0  # Total documents in training data\n",
    "\n",
    "    def preprocess(self, text):\n",
    "        \"\"\"Lowercase & remove non-alphabetic characters\"\"\"\n",
    "        text = text.lower()\n",
    "        words = re.findall(r'\\b[a-z]+\\b', text)  # Extract words and put into list\n",
    "        return words\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        \"\"\"Train the model using word frequencies\"\"\"\n",
    "        self.total_docs = len(y_train) # Number of training documents\n",
    "\n",
    "        for text, label in zip(X_train, y_train):\n",
    "            words = self.preprocess(text)\n",
    "            self.class_counts[label] += 1  # Count class occurrences\n",
    "            for word in words:\n",
    "                self.word_counts[label][word] += 1  # Count word occurrences per class\n",
    "                self.vocab.add(word)  # Add word to vocabulary\n",
    "\n",
    "    def predict(self, text):\n",
    "        \"\"\"Predict the class of a new document\"\"\"\n",
    "        words = self.preprocess(text)\n",
    "        class_probs = {}  # Store log probabilities for each class\n",
    "\n",
    "        for label in self.class_counts: #Iterate Over Each Class and Compute Probabilities\n",
    "            # Compute log prior probability P(y)\n",
    "            # P(y) = (# of documents in class y) / (total documents)\n",
    "            prior = math.log(self.class_counts[label] / self.total_docs)\n",
    "\n",
    "            # Compute log likelihood P(x|y) with Laplace smoothing\n",
    "            #total_words_in_class: Total word occurrences in a class.\n",
    "            #vocab_size: Number of unique words in training data.\n",
    "            total_words_in_class = sum(self.word_counts[label].values())\n",
    "            vocab_size = len(self.vocab)\n",
    "\n",
    "            #Laplace Smoothing (+1) ensures no word has zero probability.\n",
    "            #P(word | class)，testing sample words的每一个word都要加起来\n",
    "\n",
    "            for word in words:\n",
    "                word_freq = self.word_counts[label].get(word, 0) + 1  # Laplace smoothing\n",
    "                word_prob = word_freq / (total_words_in_class + vocab_size)\n",
    "                #Since every class uses the same vocab_size, it acts as a constant denominator across all class probability calculations. When comparing class scores, constants cancel out, meaning removing vocab_size does not affect the ranking of classes\n",
    "                prior += math.log(word_prob)  # Sum log probabilities\n",
    "\n",
    "            class_probs[label] = prior\n",
    "\n",
    "        return max(class_probs, key=class_probs.get)  # Return the class with the highest probability\n",
    "\n",
    "# === Example Usage ===\n",
    "X_train = [\n",
    "    \"I love programming in Python\",\n",
    "    \"Python is great for machine learning\",\n",
    "    \"I enjoy machine learning and AI\",\n",
    "    \"Cats are amazing pets\",\n",
    "    \"I love my pet dog\",\n",
    "    \"Dogs and cats are friendly animals\"\n",
    "]\n",
    "y_train = [\"tech\", \"tech\", \"tech\", \"pets\", \"pets\", \"pets\"]\n",
    "\n",
    "# Train the model\n",
    "nb_classifier = NaiveBayesTextClassifier()\n",
    "nb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict new examples\n",
    "print(nb_classifier.predict(\"I enjoy coding in Python\"))  # Likely: \"tech\"\n",
    "print(nb_classifier.predict(\"My cat is adorable\"))  # Likely: \"pets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tech', 'health', 'finance']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_probs = {\"tech\":1, \"finance\": 1.2, \"health\": 1.3}\n",
    "sorted_dict = sorted(class_probs, reverse= True)\n",
    "sorted_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [\n",
    "    \"I love coding\",\n",
    "    \"Python is amazing\",\n",
    "    \"I hate bugs\",\n",
    "    \"Debugging is hard\"\n",
    "]\n",
    "\n",
    "y_train = [\"positive\", \"positive\", \"negative\", \"negative\"]\n",
    "\n",
    "# After calling fit(), the model stores:\n",
    "#Class counts (how many examples per class)\n",
    "# self.class_counts\n",
    "{'positive': 2, 'negative': 2}\n",
    "\n",
    "#\t2.\tWord counts per class\n",
    "# self.word_counts\n",
    "{\n",
    "    'positive': {'I': 1, 'love': 1, 'coding': 1, 'Python': 1, 'is': 1, 'amazing': 1},\n",
    "    'negative': {'I': 1, 'hate': 1, 'bugs': 1, 'Debugging': 1, 'is': 1, 'hard': 1}\n",
    "}\n",
    "#\t3.\tVocabulary (unique words across all texts)\n",
    "{'I', 'love', 'coding', 'Python', 'is', 'amazing', 'hate', 'bugs', 'Debugging', 'hard'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.word_counts['positive'] = {'i': 1, 'love': 1, 'coding': 1, 'python': 1, 'is': 1, 'amazing': 1}\n",
    "total_words_in_class = 6\n",
    "vocab_size = 10\n",
    "\n",
    "#\tFor \"love\":\n",
    "word_freq = 1 + 1 = 2\n",
    "word_prob = 2 / (6 + 10) = 2/16 = 0.125\n",
    "log_prob += log(0.125) = -2.08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' likelihood 是怎么算的\n",
    "P(word | class) = \\frac{\\text{word count in class} + 1}{\\text{total words in class} + \\text{vocabulary size}}\n",
    "\n",
    "If we calculate probabilities without smoothing, the probability of a word given a class is:\n",
    "\n",
    "这个容易理解；\n",
    "P(word | class) = \\frac{\\text{word count in class}}{\\text{total words in class}}\n",
    "\n",
    "\n",
    "However, if a word never appeared in the training data for a particular class, its count is zero, which results in:\n",
    "\n",
    "\n",
    "P(word | class) = \\frac{0}{\\text{total words in class}} = 0\n",
    "\n",
    "\n",
    "Since Naive Bayes multiplies probabilities, having even one zero probability makes the entire class probability zero. This can wrongly eliminate the class from consideration.\n",
    "\n",
    "Laplace Smoothing fixes this by adding 1 to each word count.\n",
    "\n",
    "Since every class uses the same vocab_size, it acts as a constant denominator across all class probability calculations. When comparing class scores, constants cancel out, meaning removing vocab_size does not affect the ranking of classes\n",
    "\n",
    "\n",
    "Numerator:  \\text{word count in class} + 1\n",
    "\t•\tIf a word appears  N  times in a class, we count it as  N+1 .\n",
    "\t•\tIf a word never appeared ( N = 0 ), we still give it a small probability ( 0 + 1 = 1 ).\n",
    "\t•\tThis prevents zero probabilities.\n",
    "\n",
    "Denominator:  \\text{total words in class} + \\text{vocabulary size}\n",
    "\t•\tThe total count of all words in the class is increased by the vocabulary size (the number of unique words in all training data).\n",
    "\t•\tThis ensures that the probabilities remain valid and still sum to 1.\n",
    "\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
