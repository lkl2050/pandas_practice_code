{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single Neuron with Backpropagation\n",
    "Write a Python function that simulates a single neuron with sigmoid activation, and implements backpropagation to update the neuron's weights and bias. The function should take a list of feature vectors, associated true binary labels, initial weights, initial bias, a learning rate, and the number of epochs. The function should update the weights and bias using gradient descent based on the MSE loss, and return the updated weights, bias, and a list of MSE values for each epoch, each rounded to four decimal places.\n",
    "\n",
    "Example:\n",
    "Input:\n",
    "features = [[1.0, 2.0], [2.0, 1.0], [-1.0, -2.0]], labels = [1, 0, 0], initial_weights = [0.1, -0.2], initial_bias = 0.0, learning_rate = 0.1, epochs = 2\n",
    "Output:\n",
    "updated_weights = [0.1036, -0.1425], updated_bias = -0.0167, mse_values = [0.3033, 0.2942]\n",
    "Reasoning:\n",
    "The neuron receives feature vectors and computes predictions using the sigmoid activation. Based on the predictions and true labels, the gradients of MSE loss with respect to weights and bias are computed and used to update the model parameters across epochs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.48896563  0.89242284] -0.5058885497167279 [0.3033228034139421, 0.2942232621822798, 0.28558133945119507, 0.2774071764961471, 0.26970056278816107, 0.2624525701137394, 0.2556473456548066, 0.24926389449754993, 0.24327772692027094, 0.2376622928253055, 0.2323901671242655, 0.22743398182494132, 0.22276712214551223, 0.2183642163152234, 0.21420145385885103, 0.21025676732419143, 0.20650990954862233, 0.2029424541748781, 0.19953774225606083, 0.196280793079425, 0.19315819313448826, 0.19015797359753372, 0.18726948381955819, 0.1844832660362877, 0.18179093478295724, 0.17918506320021085, 0.17665907747199558, 0.17420715996351055, 0.17182416116168517, 0.16950552020947304, 0.167247193626997, 0.1650455916953939, 0.16289752191908377, 0.16080013896129058, 0.15875090045296691, 0.1567475280974395, 0.15478797352546103, 0.15287038839328954, 0.1509930982567664, 0.14915457979501365, 0.1473534409969651, 0.14558840396165762, 0.1438582899985562, 0.14216200674695342, 0.14049853706360216, 0.13886692945524248, 0.13726628985768657, 0.13569577458575002, 0.13415458429873686, 0.13264195884455268, 0.13115717286200276, 0.12969953203558737, 0.12826836991028284, 0.1268630451855398, 0.1254829394181651, 0.12412745507301075, 0.12279601386858004, 0.12148805537188384, 0.1202030358032395, 0.1189404270172806, 0.11769971563133196, 0.11648040227656169, 0.11528200095103147, 0.11410403845698021, 0.11294605390745503, 0.11180759828980208, 0.11068823407558372, 0.10958753486825161, 0.10850508508140615, 0.10744047964174817, 0.10639332371190781, 0.10536323242924506, 0.10434983065747867, 0.10335275274863624, 0.10237164231334772, 0.10140615199794006, 0.10045594326714775, 0.09952068619154787, 0.098600059239061, 0.09769374907005024, 0.09680145033569841, 0.09592286547946045, 0.09505770454147917, 0.09420568496591768, 0.09336653141121386, 0.09253997556329556, 0.09172575595181942, 0.09092361776950968, 0.09013331269467982, 0.08935459871702267, 0.08858723996674846, 0.08783100654714736, 0.08708567437064207, 0.08635102499838782, 0.08562684548346516, 0.0849129282177008, 0.08420907078213961, 0.08351507580118074, 0.08283075080037926, 0.08215590806790558]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int):\n",
    "    \"\"\"\n",
    "    Train a single neuron using backpropagation with MSE loss.\n",
    "\n",
    "    :param features: NumPy array of shape (n_samples, n_features)\n",
    "    :param labels: NumPy array of shape (n_samples,) with binary labels (0 or 1)\n",
    "    :param initial_weights: NumPy array of initial weights\n",
    "    :param initial_bias: Initial bias value\n",
    "    :param learning_rate: Learning rate for gradient descent\n",
    "    :param epochs: Number of training epochs\n",
    "    :return: Updated weights, updated bias, list of MSE loss values per epoch (all rounded to 4 decimals)\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert inputs to NumPy arrays\n",
    "    features = np.array(features, dtype=np.float64)\n",
    "    labels = np.array(labels, dtype=np.float64)\n",
    "    weights = np.array(initial_weights, dtype=np.float64)\n",
    "    bias = float(initial_bias)\n",
    "\n",
    "    mse_values = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        #formward pass\n",
    "        predicted = features @ weights + bias \n",
    "        predicted = sigmoid(predicted)\n",
    "\n",
    "        error = predicted - labels\n",
    "        mse = np.mean(error**2)\n",
    "        mse_values.append(mse)\n",
    "\n",
    "        d_grad = 2 * error * predicted * (1-predicted)\n",
    "        dw = features.T @ d_grad/len(labels)\n",
    "        db = np.mean(d_grad)\n",
    "\n",
    "        weights-= dw * learning_rate\n",
    "        bias-= db * learning_rate\n",
    "    \n",
    "    return weights, bias, mse_values \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "features = [[1.0, 2.0], [2.0, 1.0], [-1.0, -2.0]]\n",
    "labels = [1, 0, 0]\n",
    "initial_weights = [0.1, -0.2]\n",
    "initial_bias = 0.0\n",
    "learning_rate = 0.1\n",
    "epochs = 100\n",
    "\n",
    "updated_weights, updated_bias, mse_values = train_neuron(features, labels, initial_weights, initial_bias, learning_rate, epochs)\n",
    "print(updated_weights, updated_bias, mse_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are using Mean Squared Error (MSE) with sigmoid output:\n",
    "\n",
    "L = \\frac{1}{N} \\sum_{i=1}^N ( \\hat{y}_i - y_i )^2\n",
    "\n",
    "where\n",
    "\t•\t\\hat{y}_i = \\sigma(z_i) = prediction,\n",
    "\t•\tz_i = w \\cdot x_i + b = linear output.\n",
    "\n",
    "⸻\n",
    "\n",
    "2. Gradient chain rule\n",
    "\n",
    "We want derivative of the loss w.r.t. the weights.\n",
    "That requires:\n",
    "\n",
    "\\frac{\\partial L}{\\partial w} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z} \\cdot \\frac{\\partial z}{\\partial w}\n",
    "\n",
    "⸻\n",
    "\n",
    "Step A. Derivative of loss w.r.t prediction\n",
    "\n",
    "\\frac{\\partial L}{\\partial \\hat{y}} = 2(\\hat{y} - y)\n",
    "\n",
    "That’s where 2 * errors comes from (errors = predictions - labels).\n",
    "\n",
    "⸻\n",
    "\n",
    "Step B. Derivative of sigmoid\n",
    "\n",
    "\\frac{\\partial \\hat{y}}{\\partial z} = \\hat{y}(1 - \\hat{y})\n",
    "\n",
    "That’s where predictions * (1 - predictions) comes from.\n",
    "\n",
    "⸻\n",
    "\n",
    "Step C. Combine them\n",
    "\n",
    "So the gradient w.r.t. the linear output z:\n",
    "\n",
    "\\frac{\\partial L}{\\partial z} = 2(\\hat{y} - y) \\cdot \\hat{y}(1 - \\hat{y})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
