{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "554bddc9",
   "metadata": {},
   "source": [
    "Create a function named sparse_window_attention that computes sparse attention over long sequences by sliding a fixed-radius window across the sequence.\n",
    "\n",
    "â¢ The parameter window_size represents the radius w of the window.\n",
    "\n",
    "For a token at index i, attend only to tokens whose indices are within max(0, i - w) through min(seq_len - 1, i + w), inclusive.\n",
    "Tokens near the beginning or end of the sequence simply have smaller windows; no padding is added.\n",
    "â¢ Inputs\n",
    "\n",
    "Q, K, V: NumPy arrays with shapes (seq_len, d_k) for Q and K, and (seq_len, d_v) for V.\n",
    "window_size: integer window radius.\n",
    "scale_factor (optional): value used to scale dot-product scores; if None, default to sqrt(d_k).\n",
    "â¢ Output\n",
    "\n",
    "A NumPy array of shape (seq_len, d_v) containing the attention results.\n",
    "Example:\n",
    "Input:\n",
    "import numpy as np\n",
    "Q = np.array([[1.0], [1.0], [1.0]])\n",
    "K = np.array([[1.0], [1.0], [1.0]])\n",
    "V = np.array([[1.0], [2.0], [3.0]])\n",
    "print(sparse_window_attention(Q, K, V, 1))\n",
    "Output:\n",
    "[[1.5] [2. ] [2.5]]\n",
    "Reasoning:\n",
    "The sparse_window_attention function processes each query in the input Q by computing attention scores only with keys in K within a window of size 1 (i.e., the current position and one adjacent position on each side), then applies softmax to these scores to derive weights for the corresponding values in V. For the given input arrays, this results in the output where the first element is the weighted average of V[0] and V[1] (yielding 1.5), the second is the average of all elements in V (yielding 2.0), and the third is the average of V[1] and V[2] (yielding 2.5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cca8339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.5]\n",
      " [2. ]\n",
      " [2.5]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sparse_window_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, window_size: int, scale_factor: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Sparse sliding-window attention (radius = window_size).\n",
    "    Q,K: (L, d_k), V: (L, d_v). Returns (L, d_v).\n",
    "    For token i, attend to j in [max(0,i-w), min(L-1,i+w)].\n",
    "    \"\"\"\n",
    "    L, d_k = Q.shape # L is the sequence length\n",
    "    d_v = V.shape[1]\n",
    "    if scale_factor is None:\n",
    "        scale = np.sqrt(d_k)\n",
    "    else:\n",
    "        scale = float(scale_factor)\n",
    "\n",
    "    out = np.zeros((L, d_v), dtype=float)\n",
    "\n",
    "    for i in range(L):\n",
    "        lo = max(0, i - window_size)\n",
    "        hi = min(L - 1, i + window_size) + 1  # slice end is exclusive\n",
    "        K_win = K[lo:hi]                     # (W, d_k)\n",
    "        V_win = V[lo:hi]                     # (W, d_v)\n",
    "\n",
    "        # scores: (W,)\n",
    "        scores = (K_win @ Q[i])/ scale\n",
    "        # stable softmax\n",
    "        scores -= scores.max()\n",
    "        weights = np.exp(scores)\n",
    "        weights /= weights.sum()\n",
    "\n",
    "        out[i] = weights @ V_win            # (d_v,)\n",
    "    return out\n",
    "\n",
    "# Example\n",
    "if __name__ == \"__main__\":\n",
    "    Q = np.array([[1.0],[1.0],[1.0]])\n",
    "    K = np.array([[1.0],[1.0],[1.0]])\n",
    "    V = np.array([[1.0],[2.0],[3.0]])\n",
    "    print(sparse_window_attention(Q, K, V, 1, None))\n",
    "    # [[1.5]\n",
    "    #  [2. ]\n",
    "    #  [2.5]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
