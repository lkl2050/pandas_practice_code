{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement masked self-attention, a variation of the attention mechanism used in sequence modeling tasks such as text generation. Your task is to compute masked self-attention using query (Q), key (K), value (V) matrices and an attention mask.\n",
    "\n",
    "Example:\n",
    "Input:\n",
    "masked_attention(Q, K, V, mask)\n",
    "Output:\n",
    "[[547. 490. 399. 495. 485. 439. 645. 393.]\n",
    " [547. 490. 399. 495. 485. 439. 645. 393.]\n",
    " [471. 472. 429. 538. 377. 450. 531. 362.]\n",
    " [471. 472. 429. 538. 377. 450. 531. 362.]\n",
    " [471. 472. 429. 538. 377. 450. 531. 362.]\n",
    " [471. 472. 429. 538. 377. 450. 531. 362.]]\n",
    "Reasoning:\n",
    "The function computes self-attention by applying a mask to restrict information flow, ensuring causal dependencies are maintained.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray):\n",
    "    Q = np.dot(X, W_q)\n",
    "    K = np.dot(X, W_k)\n",
    "    V = np.dot(X, W_v)\n",
    "    return Q, K, V\n",
    "\n",
    "def masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n",
    "    d_k = Q.shape[1]\n",
    "    scores = np.matmul(Q, K.T) / np.sqrt(d_k)\n",
    "    scores = scores + mask  # Apply mask\n",
    "    attention_weights = np.exp(scores - np.max(scores, axis=1, keepdims=True)) #Subtracts max for numerical stability\n",
    "    attention_weights = attention_weights / np.sum(attention_weights, axis=1, keepdims=True)\n",
    "    return np.matmul(attention_weights, V)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
