{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement masked self-attention, a variation of the attention mechanism used in sequence modeling tasks such as text generation. Your task is to compute masked self-attention using query (Q), key (K), value (V) matrices and an attention mask.\n",
    "\n",
    "Example:\n",
    "Input:\n",
    "masked_attention(Q, K, V, mask)\n",
    "Output:\n",
    "[[547. 490. 399. 495. 485. 439. 645. 393.]\n",
    " [547. 490. 399. 495. 485. 439. 645. 393.]\n",
    " [471. 472. 429. 538. 377. 450. 531. 362.]\n",
    " [471. 472. 429. 538. 377. 450. 531. 362.]\n",
    " [471. 472. 429. 538. 377. 450. 531. 362.]\n",
    " [471. 472. 429. 538. 377. 450. 531. 362.]]\n",
    "Reasoning:\n",
    "The function computes self-attention by applying a mask to restrict information flow, ensuring causal dependencies are maintained.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Q,K,V: shape (T,D) or (B,T,D)\n",
    "    mask: same score shape as the attention scores:\n",
    "          (T,T) for 2D inputs, or (B,1,T,T) or (B,T,T) for batched (will broadcast).\n",
    "          Values: 0 for allowed, -1e9 (or -np.inf) for masked.\n",
    "    Returns: same leading dims as Q, last dim = D (i.e., (T,D) or (B,T,D))\n",
    "    \"\"\"\n",
    "    # handle 2D -> 3D for unified math\n",
    "    added_batch = False\n",
    "    if Q.ndim == 2:\n",
    "        Q = Q[None, ...]; K = K[None, ...]; V = V[None, ...]\n",
    "        added_batch = True\n",
    "\n",
    "    B,T,D = Q.shape\n",
    "    d_k = D\n",
    "\n",
    "    # scores: (B,T,T)\n",
    "    scores = np.matmul(Q, np.swapaxes(K, -1, -2)) / np.sqrt(d_k)\n",
    "\n",
    "    # apply mask (broadcast OK). mask zeros keep scores; large negative blocks\n",
    "    scores = scores + mask  # e.g., mask shape (T,T) or (B,1,T,T) or (B,T,T)\n",
    "\n",
    "    # stable softmax over last dim (keys)\n",
    "    scores_shift = scores - np.max(scores, axis=-1, keepdims=True)\n",
    "    attn = np.exp(scores_shift)\n",
    "    attn /= np.sum(attn, axis=-1, keepdims=True)\n",
    "\n",
    "    # output: (B,T,D)\n",
    "    out = np.matmul(attn, V)\n",
    "\n",
    "    if added_batch:\n",
    "        out = out[0]\n",
    "    return out\n",
    "\n",
    "def causal_mask(T: int, *, batch: int | None = None):\n",
    "    \"\"\"\n",
    "    Lower-triangular (causal) mask.\n",
    "    Returns (T,T) if batch=None, else (batch,1,T,T) for per-batch broadcast.\n",
    "    \"\"\"\n",
    "    m = np.triu(np.ones((T,T), dtype=np.float64), k=1)  # 1 above diagonal\n",
    "    m = np.where(m==1, -1e9, 0.0)\n",
    "    if batch is None:\n",
    "        return m\n",
    "    return m[None, None, ...].repeat(batch, axis=0)\n",
    "\n",
    "# Example (2D, causal)\n",
    "T,D = 6, 8\n",
    "np.random.seed(0)\n",
    "Q = np.random.randn(T,D)\n",
    "K = np.random.randn(T,D)\n",
    "V = np.random.randn(T,D)\n",
    "mask = causal_mask(T)                # (T,T) with 0 or -1e9\n",
    "out = masked_attention(Q,K,V,mask)\n",
    "print(out.shape)  # (6,8)\n",
    "\n",
    "# Example (batched)\n",
    "B,T,D = 2, 6, 8\n",
    "Qb = np.random.randn(B,T,D); Kb = np.random.randn(B,T,D); Vb = np.random.randn(B,T,D)\n",
    "mask_b = causal_mask(T, batch=B)     # (B,1,T,T), broadcasts over heads if you add them later\n",
    "out_b = masked_attention(Qb,Kb,Vb,mask_b)\n",
    "print(out_b.shape)  # (2,6,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def masked_attention(Q, K, V, mask):\n",
    "    # Q,K,V: (T,D); mask: (T,T) with 0 for allowed, -1e9 (or -np.inf) to block\n",
    "    d_k = Q.shape[1]\n",
    "    scores = (Q @ K.T) / np.sqrt(d_k)            # (T,T)\n",
    "    scores = scores + mask                       # apply mask\n",
    "    scores -= np.max(scores, axis=1, keepdims=True)  # stable softmax\n",
    "    attn = np.exp(scores)\n",
    "    attn /= np.sum(attn, axis=1, keepdims=True)  # row-wise softmax\n",
    "    return attn @ V                              # (T,D)\n",
    "\n",
    "def causal_mask(T):\n",
    "    # disallow attending to future positions (upper triangle)\n",
    "    m = np.triu(np.ones((T,T), dtype=np.float64), k=1)\n",
    "    return np.where(m==1, -1e9, 0.0)\n",
    "\n",
    "# example\n",
    "np.random.seed(0)\n",
    "T,D = 6,8\n",
    "Q = np.random.randn(T,D)\n",
    "K = np.random.randn(T,D)\n",
    "V = np.random.randn(T,D)\n",
    "mask = causal_mask(T)\n",
    "out = masked_attention(Q,K,V,mask)\n",
    "print(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
