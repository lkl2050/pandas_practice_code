{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79aaf0f4",
   "metadata": {},
   "source": [
    "Implement the multi-head attention mechanism, a critical component of transformer models. Given Query (Q), Key (K), and Value (V) matrices, compute the attention outputs for multiple heads and concatenate the results.\n",
    "\n",
    "Example:\n",
    "Input:\n",
    "Q = np.array([[1, 0], [0, 1]]), K = np.array([[1, 0], [0, 1]]), V = np.array([[1, 0], [0, 1]]), n_heads = 2\n",
    "Output:\n",
    "[[1., 0.], [0., 1.]]\n",
    "Reasoning:\n",
    "Multi-head attention is computed for 2 heads using the input Q, K, and V matrices. The resulting outputs for each head are concatenated to form the final attention output.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0678ee66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.73105858 0.5       ]\n",
      " [0.5        0.73105858]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def _softmax(x, axis=-1):\n",
    "    x = x - np.max(x, axis=axis, keepdims=True)  # stable\n",
    "    e = np.exp(x)\n",
    "    return e / np.sum(e, axis=axis, keepdims=True)\n",
    "\n",
    "def compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray):\n",
    "    \"\"\"X: (T, D), W_*: (D, D). Returns Q,K,V each (T, D).\"\"\"\n",
    "    return X @ W_q, X @ W_k, X @ W_v\n",
    "\n",
    "def self_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray):\n",
    "    \"\"\"Single-head scaled dot-product attention for (T, d).\"\"\"\n",
    "    d_k = Q.shape[-1]\n",
    "    scores = (Q @ K.T) / np.sqrt(d_k)          # (T, T)\n",
    "    attn = _softmax(scores, axis=-1)           # row-wise softmax\n",
    "    return attn @ V                             # (T, d)\n",
    "\n",
    "def multi_head_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, n_heads: int):\n",
    "    \"\"\"\n",
    "    Q,K,V: (T, D). n_heads must divide D.\n",
    "    Returns concatenated head outputs: (T, D).\n",
    "    \"\"\"\n",
    "    T, D = Q.shape\n",
    "    assert D % n_heads == 0, \"D must be divisible by n_heads\"\n",
    "    d_h = D // n_heads\n",
    "\n",
    "    # reshape to (H, T, d_h)\n",
    "    def split_heads(X):\n",
    "        return X.reshape(T, n_heads, d_h).transpose(1, 0, 2)\n",
    "\n",
    "    Qh, Kh, Vh = map(split_heads, (Q, K, V))   # each (H, T, d_h)\n",
    "\n",
    "    # compute attention per head\n",
    "    outs = []\n",
    "    for h in range(n_heads):\n",
    "        out_h = self_attention(Qh[h], Kh[h], Vh[h])  # (T, d_h)\n",
    "        outs.append(out_h)\n",
    "\n",
    "    # concat heads: (T, D)\n",
    "    return np.concatenate(outs, axis=-1)\n",
    "\n",
    "# Example\n",
    "if __name__ == \"__main__\":\n",
    "    Q = np.array([[1, 0], [0, 1]], dtype=float)\n",
    "    K = np.array([[1, 0], [0, 1]], dtype=float)\n",
    "    V = np.array([[1, 0], [0, 1]], dtype=float)\n",
    "    print(multi_head_attention(Q, K, V, n_heads=2))\n",
    "    # [[1. 0.]\n",
    "    #  [0. 1.]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
