{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: Implement the Self-Attention Mechanism\n",
    "Your task is to implement the self-attention mechanism, which is a fundamental component of transformer models, widely used in natural language processing and computer vision tasks. The self-attention mechanism allows a model to dynamically focus on different parts of the input sequence when generating a contextualized representation.\n",
    "\n",
    "Your function should return the self-attention output as a numpy array.\n",
    "\n",
    "Example:\n",
    "Input:\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([[1, 0], [0, 1]])\n",
    "W_q = np.array([[1, 0], [0, 1]])\n",
    "W_k = np.array([[1, 0], [0, 1]])\n",
    "W_v = np.array([[1, 2], [3, 4]])\n",
    "\n",
    "Q, K, V = compute_qkv(X, W_q, W_k, W_v)\n",
    "output = self_attention(Q, K, V)\n",
    "\n",
    "print(output)\n",
    "Output:\n",
    "[[1.660477 2.660477]\n",
    "[2.339523 3.339523]]\n",
    "Reasoning:\n",
    "The self-attention mechanism calculates the attention scores for each input, determining how much focus to put on other inputs when generating a contextualized representation. The output is the weighted sum of the values based on the attention scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided code implements self-attention, a mechanism in transformer models that allows the network to focus on different parts of the input sequence when computing contextualized representations\n",
    "\n",
    "\t•\tWhy do we need self-attention?\n",
    "\t•\tInstead of treating tokens independently, self-attention allows each token to “attend” to all other tokens and extract useful contextual information.\n",
    "\t•\tWhy is softmax used?\n",
    "\t•\tIt helps distribute attention across all tokens so that the focus is not entirely on one token.\n",
    "\t•\tWhy multiply Q and K.T?\n",
    "\t•\tThis measures how similar each token’s query (what it wants) is to another token’s key (what it has)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.6604769 2.6604769]\n",
      " [2.3395231 3.3395231]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def compute_qkv(X, W_q, W_k, W_v):\n",
    "    \"\"\"\n",
    "    Compute the query (Q), key (K), and value (V) matrices.\n",
    "\n",
    "    :param X: Input matrix (sequence_length, embedding_size)\n",
    "    :param W_q: Query weight matrix (embedding_size, hidden_size)\n",
    "    :param W_k: Key weight matrix (embedding_size, hidden_size)\n",
    "    :param W_v: Value weight matrix (embedding_size, hidden_size)\n",
    "    :return: Q, K, V matrices\n",
    "    \"\"\"\n",
    "    Q = np.dot(X, W_q)\n",
    "    K = np.dot(X, W_k)\n",
    "    V = np.dot(X, W_v)\n",
    "    return Q, K, V\n",
    "\n",
    "def self_attention(Q, K, V):\n",
    "    d_k = K.shape[-1]  # Dimension of key vectors\n",
    "    #shape[-1] selects the last dimension, regardless of how many dimensions exist\n",
    "    # ensures we always extract the embedding size (number of features per token)\n",
    "\n",
    "    scores = np.dot(Q, K.T) / math.sqrt(d_k)  # Scaled dot-product attention\n",
    "    attention_weights = np.exp(scores) / np.sum(np.exp(scores), axis=1, keepdims=True)  # Softmax\n",
    "    output = np.dot(attention_weights, V)  # Weighted sum of values\n",
    "    return output\n",
    "# axis=1 means we are summing along each row (across columns).\n",
    "# This ensures that each row in attention_weights sums to 1, making it a valid probability distribution.\n",
    "\n",
    "# keepdims=True ensures that the original shape is maintained.\n",
    "#Without keepdims=True, np.sum() would reduce the dimension.\n",
    "\n",
    "# Example usage\n",
    "X = np.array([[1, 0], [0, 1]])\n",
    "W_q = np.array([[1, 0], [0, 1]])\n",
    "W_k = np.array([[1, 0], [0, 1]])\n",
    "W_v = np.array([[1, 2], [3, 4]])\n",
    "\n",
    "Q, K, V = compute_qkv(X, W_q, W_k, W_v)\n",
    "output = self_attention(Q, K, V)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
