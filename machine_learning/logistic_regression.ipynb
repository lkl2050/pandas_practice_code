{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff91797a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0  loss=0.693147\n",
      "iter 100  loss=0.285145\n",
      "iter 200  loss=0.167200\n",
      "iter 300  loss=0.116095\n",
      "iter 400  loss=0.088294\n",
      "iter 500  loss=0.071000\n",
      "iter 600  loss=0.059265\n",
      "iter 700  loss=0.050804\n",
      "iter 800  loss=0.044427\n",
      "iter 900  loss=0.039454\n",
      "iter 1000  loss=0.035471\n",
      "iter 1100  loss=0.032211\n",
      "iter 1200  loss=0.029494\n",
      "iter 1300  loss=0.027197\n",
      "iter 1400  loss=0.025229\n",
      "iter 1500  loss=0.023524\n",
      "iter 1600  loss=0.022034\n",
      "iter 1700  loss=0.020721\n",
      "iter 1800  loss=0.019554\n",
      "iter 1900  loss=0.018511\n",
      "iter 1999  loss=0.017582\n",
      "w: [-5.46847428  3.31519672] b: 3.5642522460820723\n",
      "proba: [0.99967824 0.99999886 0.0200959 ]\n",
      "pred: [1 1 0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, num_iters=1000, lr=1e-2, tol=1e-6, verbose=False):\n",
    "        self.num_iters = num_iters\n",
    "        self.lr = lr\n",
    "        self.tol = tol\n",
    "        self.verbose = verbose\n",
    "        self.w = None\n",
    "        self.b = 0.0\n",
    "\n",
    "    @staticmethod\n",
    "    def _sigmoid(z):\n",
    "        # stable sigmoid\n",
    "        z = np.clip(z, -40, 40)\n",
    "        return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "    @staticmethod\n",
    "    def _bce_loss(y, p):\n",
    "        eps = 1e-12\n",
    "        p = np.clip(p, eps, 1 - eps)\n",
    "        return -np.mean(y * np.log(p) + (1 - y) * np.log(1 - p))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        N, F = X.shape\n",
    "        self.w = np.zeros(F)\n",
    "        self.b = 0.0\n",
    "\n",
    "        prev_loss = np.inf\n",
    "        for t in range(self.num_iters):\n",
    "            z = X @ self.w + self.b\n",
    "            p = self._sigmoid(z)\n",
    "\n",
    "            loss = self._bce_loss(y, p)\n",
    "\n",
    "            # gradients of BCE wrt w,b\n",
    "            err = (p - y)  # (N,)\n",
    "            dw = (X.T @ err) / N\n",
    "            db = np.mean(err)\n",
    "\n",
    "            # gradient descent\n",
    "            self.w -= self.lr * dw\n",
    "            self.b -= self.lr * db\n",
    "\n",
    "            if self.verbose and (t % 100 == 0 or t == self.num_iters - 1):\n",
    "                print(f\"iter {t}  loss={loss:.6f}\")\n",
    "\n",
    "            # early stopping when improvement is tiny\n",
    "            if prev_loss - loss < self.tol:\n",
    "                break\n",
    "            prev_loss = loss\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self._sigmoid(X @ self.w + self.b)\n",
    "\n",
    "    def predict(self, X, threshold=0.5):\n",
    "        return (self.predict_proba(X) >= threshold).astype(int)\n",
    "\n",
    "# Example\n",
    "X_train = np.array([[1,2],[3,5],[5,6]])\n",
    "y_train = np.array([1,1,0])\n",
    "\n",
    "X_test  = np.array([[1,3],[3,8],[5,6]])\n",
    "\n",
    "model = LogisticRegression(num_iters=2000, lr=0.1, verbose=True)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"w:\", model.w, \"b:\", model.b)\n",
    "print(\"proba:\", model.predict_proba(X_test))\n",
    "print(\"pred:\", model.predict(X_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
