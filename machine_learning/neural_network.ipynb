{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69a5cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# data: two Gaussian blobs\n",
    "np.random.seed(0)\n",
    "N, D, H = 400, 2, 16\n",
    "X_pos = np.random.randn(N//2, D) + np.array([2.0, 2.0])\n",
    "X_neg = np.random.randn(N//2, D) + np.array([-2.0, -2.0])\n",
    "X = np.vstack([X_pos, X_neg])                       # (N,2)\n",
    "y = np.hstack([np.ones(N//2), np.zeros(N//2)])      # (N,)\n",
    "\n",
    "# utils\n",
    "def sigmoid(z):\n",
    "    z = np.clip(z, -40, 40)\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "def bce_loss(p, y):\n",
    "    eps = 1e-12\n",
    "    p = np.clip(p, eps, 1-eps)\n",
    "    return -np.mean(y*np.log(p) + (1-y)*np.log(1-p))\n",
    "\n",
    "# init (Xavier uniform)\n",
    "limit1 = 1/np.sqrt(D)\n",
    "W1 = np.random.uniform(-limit1, limit1, (D, H))\n",
    "b1 = np.zeros((1, H))\n",
    "limit2 = 1/np.sqrt(H)\n",
    "W2 = np.random.uniform(-limit2, limit2, (H, 1))\n",
    "b2 = np.zeros((1, 1))\n",
    "\n",
    "lr = 0.1\n",
    "epochs = 500\n",
    "batch = 64\n",
    "\n",
    "# training\n",
    "for ep in range(1, epochs+1):\n",
    "    idx = np.random.permutation(N)\n",
    "    Xs, ys = X[idx], y[idx]\n",
    "    total = 0.0\n",
    "    for s in range(0, N, batch):\n",
    "        e = min(s+batch, N)\n",
    "        xb = Xs[s:e]                                 # (B,2)\n",
    "        yb = ys[s:e].reshape(-1,1)                   # (B,1)\n",
    "        # forward\n",
    "        z1 = xb @ W1 + b1                            # (B,H)\n",
    "        a1 = np.maximum(0, z1)                       # ReLU\n",
    "        z2 = a1 @ W2 + b2                            # (B,1)\n",
    "        p  = sigmoid(z2)                             # prob\n",
    "        loss = bce_loss(p, yb)\n",
    "        total += loss*(e-s)\n",
    "        # backward\n",
    "        dL_dz2 = (p - yb)                            # (B,1) for BCE+sigmoid\n",
    "        dW2 = (a1.T @ dL_dz2)/len(xb)                # (H,1)\n",
    "        db2 = dL_dz2.mean(axis=0, keepdims=True)     # (1,1)\n",
    "        da1 = dL_dz2 @ W2.T                          # (B,H)\n",
    "        dz1 = da1 * (z1 > 0)                         # ReLU'\n",
    "        dW1 = (xb.T @ dz1)/len(xb)                   # (2,16)\n",
    "        db1 = dz1.mean(axis=0, keepdims=True)        # (1,16)\n",
    "        # sgd update\n",
    "        W2 -= lr*dW2; b2 -= lr*db2\n",
    "        W1 -= lr*dW1; b1 -= lr*db1\n",
    "    if ep % 100 == 0 or ep == 1 or ep == epochs:\n",
    "        print(f\"epoch {ep} loss={total/N:.4f}\")\n",
    "\n",
    "# evaluate\n",
    "z1 = X @ W1 + b1\n",
    "a1 = np.maximum(0, z1)\n",
    "p  = sigmoid(a1 @ W2 + b2).ravel()\n",
    "pred = (p >= 0.5).astype(int)\n",
    "acc = (pred == y).mean()\n",
    "print(f\"train acc={acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6724a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ---------- utilities ----------\n",
    "def one_hot(y, num_classes):\n",
    "    out = np.zeros((y.size, num_classes))\n",
    "    out[np.arange(y.size), y] = 1\n",
    "    return out\n",
    "\n",
    "# ---------- layers ----------\n",
    "class Layer:\n",
    "    def forward(self, x, training=True): raise NotImplementedError\n",
    "    def backward(self, grad_out): raise NotImplementedError\n",
    "    def params_and_grads(self): return []  # for optimizers\n",
    "\n",
    "class Dense(Layer):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        limit = 1.0/np.sqrt(in_features)\n",
    "        self.W = np.random.uniform(-limit, limit, (in_features, out_features)).astype(np.float64)\n",
    "        self.b = np.zeros((1, out_features), dtype=np.float64)\n",
    "        self.x = None\n",
    "        self.dW = np.zeros_like(self.W)\n",
    "        self.db = np.zeros_like(self.b)\n",
    "    def forward(self, x, training=True):\n",
    "        self.x = x\n",
    "        return x @ self.W + self.b\n",
    "    def backward(self, grad_out):\n",
    "        # grad_out: (N, out_features)\n",
    "        self.dW = self.x.T @ grad_out / self.x.shape[0]\n",
    "        self.db = np.mean(grad_out, axis=0, keepdims=True)\n",
    "        return grad_out @ self.W.T\n",
    "    def params_and_grads(self):\n",
    "        return [(self.W, self.dW), (self.b, self.db)]\n",
    "\n",
    "class ReLU(Layer):\n",
    "    def __init__(self): self.mask = None\n",
    "    def forward(self, x, training=True):\n",
    "        self.mask = x>0\n",
    "        return x * self.mask\n",
    "    def backward(self, grad_out):\n",
    "        return grad_out * self.mask\n",
    "\n",
    "class Sigmoid(Layer):\n",
    "    def __init__(self): self.y = None\n",
    "    def forward(self, x, training=True):\n",
    "        z = np.clip(x, -40, 40)\n",
    "        self.y = 1.0/(1.0+np.exp(-z))\n",
    "        return self.y\n",
    "    def backward(self, grad_out):\n",
    "        return grad_out * self.y*(1.0-self.y)\n",
    "\n",
    "# ---------- losses ----------\n",
    "class SoftmaxCrossEntropy:\n",
    "    def __init__(self): self.probs = None; self.y_true = None\n",
    "    def forward(self, logits, y_true):\n",
    "        # y_true one-hot or class indices\n",
    "        if y_true.ndim == 1:\n",
    "            y_true = one_hot(y_true, logits.shape[1])\n",
    "        self.y_true = y_true\n",
    "        z = logits - np.max(logits, axis=1, keepdims=True)\n",
    "        exp = np.exp(z)\n",
    "        self.probs = exp/np.sum(exp, axis=1, keepdims=True)\n",
    "        eps = 1e-12\n",
    "        loss = -np.sum(y_true*np.log(self.probs+eps))/y_true.shape[0]\n",
    "        return loss\n",
    "    def backward(self):\n",
    "        # dL/dlogits = (softmax - y)/N\n",
    "        return (self.probs - self.y_true)\n",
    "\n",
    "class BCELoss:\n",
    "    def __init__(self): self.p=None; self.y=None\n",
    "    def forward(self, p, y):\n",
    "        self.p=p; self.y=y\n",
    "        eps=1e-12\n",
    "        return -np.mean(y*np.log(p+eps)+(1-y)*np.log(1-p+eps))\n",
    "    def backward(self):\n",
    "        eps=1e-12\n",
    "        return (self.p - self.y)/((self.p+eps)*(1-self.p+eps)) * (self.p*(1-self.p))\n",
    "\n",
    "# ---------- optimizer ----------\n",
    "class SGD:\n",
    "    def __init__(self, lr=1e-1, weight_decay=0.0):\n",
    "        self.lr = lr\n",
    "        self.wd = weight_decay\n",
    "    def step(self, model):\n",
    "        for layer in model.layers:\n",
    "            for param, grad in layer.params_and_grads():\n",
    "                if self.wd!=0.0 and param.ndim==2:\n",
    "                    grad = grad + self.wd*param\n",
    "                param -= self.lr * grad\n",
    "\n",
    "# ---------- model ----------\n",
    "class Model:\n",
    "    def __init__(self, layers, loss, optimizer):\n",
    "        self.layers = layers\n",
    "        self.loss = loss\n",
    "        self.opt = optimizer\n",
    "    def forward(self, X, training=True):\n",
    "        out = X\n",
    "        for layer in self.layers:\n",
    "            out = layer.forward(out, training=training)\n",
    "        return out\n",
    "    def backward(self, grad_out):\n",
    "        grad = grad_out\n",
    "        for layer in reversed(self.layers):\n",
    "            grad = layer.backward(grad)\n",
    "    def fit(self, X, y, epochs=1000, batch_size=64, verbose=200):\n",
    "        N = X.shape[0]\n",
    "        for ep in range(1, epochs+1):\n",
    "            idx = np.random.permutation(N)\n",
    "            Xs, ys = X[idx], y[idx]\n",
    "            total_loss = 0.0\n",
    "            for start in range(0, N, batch_size):\n",
    "                end = min(start+batch_size, N)\n",
    "                xb, yb = Xs[start:end], ys[start:end]\n",
    "                logits_or_probs = self.forward(xb, training=True)\n",
    "                loss = self.loss.forward(logits_or_probs, yb)\n",
    "                total_loss += loss*(end-start)\n",
    "                grad_out = self.loss.backward()\n",
    "                self.backward(grad_out)\n",
    "                self.opt.step(self)\n",
    "            if verbose and (ep%verbose==0 or ep==1 or ep==epochs):\n",
    "                print(f\"epoch {ep} loss={total_loss/N:.4f}\")\n",
    "    def predict_proba(self, X):\n",
    "        out = self.forward(X, training=False)\n",
    "        # If last layer is sigmoid (binary), output is prob; else softmax\n",
    "        if out.ndim==1 or out.shape[1]==1:\n",
    "            # binary via sigmoid\n",
    "            z = np.clip(out, -40, 40)\n",
    "            return 1.0/(1.0+np.exp(-z))\n",
    "        z = out - np.max(out, axis=1, keepdims=True)\n",
    "        exp = np.exp(z)\n",
    "        return exp/np.sum(exp, axis=1, keepdims=True)\n",
    "    def predict(self, X, threshold=0.5):\n",
    "        proba = self.predict_proba(X)\n",
    "        if proba.ndim==2 and proba.shape[1]>1:\n",
    "            return np.argmax(proba, axis=1)\n",
    "        return (proba>=threshold).astype(int).ravel()\n",
    "\n",
    "# ---------- demo 1: multiclass (softmax) ----------\n",
    "def demo_multiclass():\n",
    "    # synthetic 3-class blobs (NumPy only)\n",
    "    np.random.seed(42)\n",
    "    N=600; C=3; D=2\n",
    "    X = np.vstack([\n",
    "        np.random.randn(N//3, D)*0.8 + np.array([0,0]),\n",
    "        np.random.randn(N//3, D)*0.8 + np.array([3,3]),\n",
    "        np.random.randn(N//3, D)*0.8 + np.array([0,4]),\n",
    "    ])\n",
    "    y = np.hstack([np.zeros(N//3,dtype=int),\n",
    "                   np.ones(N//3,dtype=int),\n",
    "                   np.full(N//3,2,dtype=int)])\n",
    "    # optional: show how Pandas could load/hold\n",
    "    df = pd.DataFrame({\"x1\":X[:,0],\"x2\":X[:,1],\"y\":y})\n",
    "    Xn = df[[\"x1\",\"x2\"]].to_numpy(); yn = df[\"y\"].to_numpy()\n",
    "    layers = [Dense(2,32), ReLU(), Dense(32,32), ReLU(), Dense(32,C)]\n",
    "    model = Model(layers, loss=SoftmaxCrossEntropy(), optimizer=SGD(lr=0.1, weight_decay=1e-4))\n",
    "    model.fit(Xn, yn, epochs=1000, batch_size=64, verbose=200)\n",
    "    preds = model.predict(Xn)\n",
    "    acc = (preds==yn).mean()\n",
    "    print(f\"train acc (multiclass) = {acc:.3f}\")\n",
    "\n",
    "# ---------- demo 2: binary (sigmoid + BCE) ----------\n",
    "def demo_binary():\n",
    "    np.random.seed(0)\n",
    "    N=400; D=2\n",
    "    X_pos = np.random.randn(N//2, D) + np.array([2.0,2.0])\n",
    "    X_neg = np.random.randn(N//2, D) + np.array([-2.0,-2.0])\n",
    "    X = np.vstack([X_pos, X_neg])\n",
    "    y = np.hstack([np.ones(N//2,dtype=int), np.zeros(N//2,dtype=int)])\n",
    "    # model: last layer 1 unit + Sigmoid, BCE loss\n",
    "    layers = [Dense(2,16), ReLU(), Dense(16,1), Sigmoid()]\n",
    "    model = Model(layers, loss=BCELoss(), optimizer=SGD(lr=0.1, weight_decay=1e-4))\n",
    "    model.fit(X, y, epochs=500, batch_size=64, verbose=100)\n",
    "    preds = model.predict(X)\n",
    "    acc = (preds==y).mean()\n",
    "    print(f\"train acc (binary) = {acc:.3f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=== Multiclass demo (Softmax CE) ===\")\n",
    "    demo_multiclass()\n",
    "    print(\"=== Binary demo (Sigmoid + BCE) ===\")\n",
    "    demo_binary()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
