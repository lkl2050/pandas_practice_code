{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 36.757080029575256\n",
      "Epoch 10, Loss: 0.19977622757534674\n",
      "Epoch 20, Loss: 0.18659541133428664\n",
      "Epoch 30, Loss: 0.17534905316806532\n",
      "Epoch 40, Loss: 0.16528948538440674\n",
      "Epoch 50, Loss: 0.15628952113809388\n",
      "Epoch 60, Loss: 0.14823631434145768\n",
      "Epoch 70, Loss: 0.14102910792648632\n",
      "Epoch 80, Loss: 0.134577933743953\n",
      "Epoch 90, Loss: 0.12880245359361447\n",
      "\n",
      "Final parameters:\n",
      "Weight: 2.0399\n",
      "Bias: 0.6807\n",
      "\n",
      "Prediction for x=5: 10.8804\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "class LinearRegression:\n",
    "    def __init__(self, learning_rate=0.01):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight = random.random()  # Initialize random weight\n",
    "        self.bias = random.random()    # Initialize random bias\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Prediction: y = wx + b\n",
    "        return self.weight * x + self.bias\n",
    "\n",
    "    def compute_loss(self, x, y_true):\n",
    "        # Mean squared error loss\n",
    "        y_pred = self.forward(x)\n",
    "        return (y_pred - y_true) ** 2\n",
    "\n",
    "    def backward(self, x, y_true):\n",
    "        # Compute gradients\n",
    "        y_pred = self.forward(x)\n",
    "        error = y_pred - y_true\n",
    "\n",
    "        # Gradient for weight: d(loss)/dw = 2(wx + b - y)x\n",
    "        #see below explanation\n",
    "        dw = 2 * error * x\n",
    "\n",
    "        # Gradient for bias: d(loss)/db = 2(wx + b - y)\n",
    "        db = 2 * error\n",
    "\n",
    "        return dw, db\n",
    "\n",
    "    def train_step(self, x, y_true):\n",
    "        # Compute gradients\n",
    "        dw, db = self.backward(x, y_true)\n",
    "\n",
    "        # Update parameters using gradient descent\n",
    "        self.weight -= self.learning_rate * dw\n",
    "        self.bias -= self.learning_rate * db\n",
    "\n",
    "    def train(self, X, y, epochs=100):\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            for x_i, y_i in zip(X, y):\n",
    "                self.train_step(x_i, y_i)\n",
    "                total_loss += self.compute_loss(x_i, y_i)\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"Epoch {epoch}, Loss: {total_loss/len(X)}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate synthetic data\n",
    "    X = [i for i in range(10)]\n",
    "    y = [2*x + 1 + random.uniform(-0.5, 0.5) for x in X]  # y = 2x + 1 + noise\n",
    "\n",
    "    # Create and train model\n",
    "    model = LinearRegression(learning_rate=0.001)\n",
    "    model.train(X, y, epochs=100)\n",
    "\n",
    "    # Print final parameters\n",
    "    print(f\"\\nFinal parameters:\")\n",
    "    print(f\"Weight: {model.weight:.4f}\")\n",
    "    print(f\"Bias: {model.bias:.4f}\")\n",
    "\n",
    "    # Test predictions\n",
    "    test_x = 5\n",
    "    prediction = model.forward(test_x)\n",
    "    print(f\"\\nPrediction for x={test_x}: {prediction:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let me break down how we derive these gradient formulas step by step:\n",
    "\n",
    "First, let's write out our equations:\n",
    "\n",
    "Prediction: ŷ = wx + b\n",
    "Loss function (MSE): L = (ŷ - y)² = (wx + b - y)²\n",
    "\n",
    "\n",
    "For gradient descent, we need ∂L/∂w and ∂L/∂b. We'll use the chain rule.\n",
    "For weight (w):\n",
    "\n",
    "Using chain rule on L = (wx + b - y)²\n",
    "∂L/∂w = 2(wx + b - y) * ∂(wx + b - y)/∂w\n",
    "∂(wx + b - y)/∂w = x\n",
    "Therefore: ∂L/∂w = 2(wx + b - y)x\n",
    "In code: dw = 2 * error * x where error = (wx + b - y)\n",
    "\n",
    "\n",
    "For bias (b):\n",
    "\n",
    "Using chain rule again on L = (wx + b - y)²\n",
    "∂L/∂b = 2(wx + b - y) * ∂(wx + b - y)/∂b\n",
    "∂(wx + b - y)/∂b = 1\n",
    "Therefore: ∂L/∂b = 2(wx + b - y)\n",
    "In code: db = 2 * error where error = (wx + b - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LinearRegression:\n",
    "    def __init__(self, n_features=3, learning_rate=0.01):\n",
    "        self.learning_rate = learning_rate\n",
    "        # Initialize random weights as a column vector\n",
    "        self.weights = np.random.rand(n_features, 1)\n",
    "        self.bias = np.random.rand()\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward pass using matrix multiplication\n",
    "        X: shape (n_samples, n_features)\n",
    "        \"\"\"\n",
    "        # Reshape X to ensure proper multiplication if X is a single sample\n",
    "        if X.ndim == 1:\n",
    "            X = X.reshape(1, -1)\n",
    "\n",
    "        # y = Xw + b\n",
    "        return np.dot(X, self.weights) + self.bias\n",
    "\n",
    "    def compute_loss(self, X, y_true):\n",
    "        \"\"\"\n",
    "        Mean squared error loss\n",
    "        \"\"\"\n",
    "        y_pred = self.forward(X)\n",
    "        # Ensure shapes match for subtraction\n",
    "        if y_true.ndim == 1:\n",
    "            y_true = y_true.reshape(-1, 1)\n",
    "        return np.mean((y_pred - y_true) ** 2)\n",
    "\n",
    "    def backward(self, X, y_true):\n",
    "        \"\"\"\n",
    "        Compute gradients\n",
    "        \"\"\"\n",
    "\n",
    "        # This checks if X is a 1-dimensional array (like a single sample with 3 features). If it is, it reshapes it into a 2D array with 1 row and the same number of columns. The -1 tells NumPy to automatically determine the appropriate size for that dimension. So a 1D array like [0.5, 0.3, 0.8] becomes a 2D array [[0.5, 0.3, 0.8]]. This ensures X has the shape (n_samples, n_features) needed for matrix multiplication.\n",
    "\n",
    "        # Similarly, this checks if y_true is a 1D array (like [1, 2, 3]). If it is, it reshapes it into a column vector by adding a second dimension, turning it into something like [[1], [2], [3]]. The shape becomes (n_samples, 1), which is the expected format for the target values in matrix calculations.\n",
    "\n",
    "        if X.ndim == 1:\n",
    "            X = X.reshape(1, -1)\n",
    "        if y_true.ndim == 1:\n",
    "            y_true = y_true.reshape(-1, 1)\n",
    "\n",
    "        y_pred = self.forward(X)\n",
    "        error = y_pred - y_true\n",
    "\n",
    "        # Gradient for weights: dL/dw = (1/m) * X^T * (Xw - y)\n",
    "        dw = (1/X.shape[0]) * np.dot(X.T, error)\n",
    "\n",
    "        # Gradient for bias: dL/db = (1/m) * sum(Xw - y)\n",
    "        db = np.mean(error)\n",
    "\n",
    "        return dw, db\n",
    "\n",
    "    def train_step(self, X, y_true):\n",
    "        \"\"\"\n",
    "        Single training step with all data\n",
    "        \"\"\"\n",
    "        dw, db = self.backward(X, y_true)\n",
    "        self.weights -= self.learning_rate * dw\n",
    "        self.bias -= self.learning_rate * db\n",
    "\n",
    "    def train(self, X, y, epochs=100):\n",
    "        \"\"\"\n",
    "        Training loop\n",
    "        \"\"\"\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            self.train_step(X, y)\n",
    "\n",
    "            # Compute and print loss periodically\n",
    "            if epoch % 10 == 0:\n",
    "                loss = self.compute_loss(X, y)\n",
    "                print(f\"Epoch {epoch}, Loss: {loss:.6f}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate synthetic data with 3 features\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    n_samples = 100\n",
    "\n",
    "    # Create feature matrix X with 3 features\n",
    "    X = np.random.rand(n_samples, 3)\n",
    "\n",
    "    # True weights and bias\n",
    "    true_weights = np.array([2.0, -1.5, 3.0]).reshape(-1, 1)\n",
    "    true_bias = 1.0\n",
    "\n",
    "    # Generate target values: y = Xw + b + noise\n",
    "    y = np.dot(X, true_weights) + true_bias + np.random.normal(0, 0.5, (n_samples, 1))\n",
    "\n",
    "    # Create and train model\n",
    "    model = LinearRegression(n_features=3, learning_rate=0.1)\n",
    "    model.train(X, y, epochs=100)\n",
    "\n",
    "    # Print final parameters\n",
    "    print(f\"\\nFinal parameters:\")\n",
    "    print(f\"Weights:\\n{model.weights.flatten()}\")\n",
    "    print(f\"Bias: {model.bias:.4f}\")\n",
    "\n",
    "    # Test predictions\n",
    "    test_X = np.array([0.5, 0.3, 0.8])\n",
    "    prediction = model.forward(test_X)[0][0]\n",
    "    expected = np.dot(test_X.reshape(1, -1), true_weights) + true_bias\n",
    "    print(f\"\\nTest input: {test_X}\")\n",
    "    print(f\"Prediction: {prediction:.4f}\")\n",
    "    print(f\"Expected (without noise): {expected[0][0]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This line is calculating the gradient of the loss function with respect to the weights (the derivative of the loss with respect to each weight parameter). Let me break it down:\n",
    "dw = (1/X.shape[0]) * np.dot(X.T, error)\n",
    "\n",
    "X.shape[0] gives the number of samples in your dataset (the number of rows in X).\n",
    "1/X.shape[0] is the scaling factor that averages the gradient across all samples.\n",
    "X.T is the transpose of the feature matrix X. If X is shape (n_samples, n_features), then X.T becomes shape (n_features, n_samples).\n",
    "error is the difference between predicted values and actual values: y_pred - y_true. This has shape (n_samples, 1).\n",
    "np.dot(X.T, error) performs matrix multiplication between the transposed feature matrix and the error vector. The resulting shape is (n_features, 1), which gives you the gradient for each weight parameter.\n",
    "\n",
    "Mathematically, this operation is computing:\n",
    "dL/dw = (1/m) * X^T * (Xw - y)\n",
    "Where:\n",
    "\n",
    "m is the number of samples\n",
    "X^T is the transpose of X\n",
    "(Xw - y) is the prediction error\n",
    "\n",
    "This gradient tells you how much each weight contributes to the error and in which direction to adjust each weight to reduce the loss. Multiplying by (1/m) averages the gradient across all training examples.\n",
    "In the context of linear regression with MSE loss, this is the exact analytical gradient formula that points in the direction of steepest descent of the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L = (1/m) * Σ(y_pred - y_true)²\n",
    "\n",
    "L = (1/m) * (Xw + b - y)^T * (Xw + b - y)\n",
    "L = (1/m) * [(Xw)^T - y^T] * [Xw - y]\n",
    "L = (1/m) * [w^T * X^T - y^T] * [Xw - y]\n",
    "L = (1/m) * [w^T * X^T * Xw - w^T * X^T * y - y^T * Xw + y^T * y]\n",
    "\n",
    "∂L/∂w = (1/m) * [2*X^T*X*w - X^T*y - X^T*y + 0]\n",
    "      = (1/m) * [2*X^T*X*w - 2*X^T*y]\n",
    "      = (1/m) * 2*X^T*(Xw - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self, n_features=3):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        self.linear = nn.Linear(n_features, 1)  # Linear layer: y = Xw + b\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.linear(X)\n",
    "\n",
    "# Generate synthetic data\n",
    "torch.manual_seed(42)  # For reproducibility\n",
    "n_samples = 100\n",
    "n_features = 3\n",
    "\n",
    "# Create feature matrix X with 3 features\n",
    "X = torch.rand(n_samples, n_features)\n",
    "\n",
    "# True weights and bias\n",
    "true_weights = torch.tensor([2.0, -1.5, 3.0]).reshape(-1, 1)\n",
    "true_bias = 1.0\n",
    "\n",
    "# Generate target values: y = Xw + b + noise\n",
    "y = X @ true_weights + true_bias + torch.randn(n_samples, 1) * 0.5\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = LinearRegression(n_features=n_features)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# Training loop\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()  # Reset gradients\n",
    "    predictions = model(X)  # Forward pass\n",
    "    loss = criterion(predictions, y)  # Compute loss\n",
    "    loss.backward()  # Backpropagation\n",
    "    optimizer.step()  # Update parameters\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item():.6f}\")\n",
    "\n",
    "# Print final parameters\n",
    "print(\"\\nFinal parameters:\")\n",
    "print(f\"Weights: {model.linear.weight.data.flatten().tolist()}\")\n",
    "print(f\"Bias: {model.linear.bias.item():.4f}\")\n",
    "\n",
    "# Test predictions\n",
    "test_X = torch.tensor([[0.5, 0.3, 0.8]])\n",
    "prediction = model(test_X).item()\n",
    "expected = (test_X @ true_weights + true_bias).item()\n",
    "print(f\"\\nTest input: {test_X.flatten().tolist()}\")\n",
    "print(f\"Prediction: {prediction:.4f}\")\n",
    "print(f\"Expected (without noise): {expected:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
