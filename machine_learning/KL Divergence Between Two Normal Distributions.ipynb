{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: Implement KL Divergence Between Two Normal Distributions\n",
    "Your task is to compute the Kullback-Leibler (KL) divergence between two normal distributions. KL divergence measures how one probability distribution differs from a second, reference probability distribution.\n",
    "\n",
    "Write a function kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q) that calculates the KL divergence between two normal distributions, where ( P \\sim N(\\mu_P, \\sigma_P^2) ) and ( Q \\sim N(\\mu_Q, \\sigma_Q^2) ).\n",
    "\n",
    "The function should return the KL divergence as a floating-point number.\n",
    "\n",
    "Example:\n",
    "Input:\n",
    "mu_p = 0.0\n",
    "sigma_p = 1.0\n",
    "mu_q = 1.0\n",
    "sigma_q = 1.0\n",
    "\n",
    "print(kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q))\n",
    "Output:\n",
    "0.5\n",
    "Reasoning:\n",
    "The KL divergence between the normal distributions ( P ) and ( Q ) with parameters ( \\mu_P = 0.0 ), ( \\sigma_P = 1.0 ) and ( \\mu_Q = 1.0 ), ( \\sigma_Q = 1.0 ) is 0.5.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n",
    "    term1 = np.log(sigma_q / sigma_p)\n",
    "    term2 = (sigma_p ** 2 + (mu_p - mu_q) ** 2) / (2 * sigma_q ** 2)\n",
    "    kl_div = term1 + term2 - 0.5\n",
    "    return kl_div\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
