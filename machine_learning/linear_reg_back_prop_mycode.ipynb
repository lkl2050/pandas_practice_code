{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 loss=1050.1278631083073\n",
      "epoch 10 loss=325.13631257594653\n",
      "epoch 20 loss=101.30460625259974\n",
      "epoch 30 loss=32.20586698346584\n",
      "epoch 40 loss=10.876741234443305\n",
      "epoch 50 loss=4.292913704745278\n",
      "epoch 60 loss=2.259352178443832\n",
      "epoch 70 loss=1.6292893488360758\n",
      "epoch 80 loss=1.431768711044012\n",
      "epoch 90 loss=1.3673688719533201\n",
      "epoch 100 loss=1.3438508436489833\n",
      "epoch 110 loss=1.3328525931634976\n",
      "epoch 120 loss=1.3256823537468276\n",
      "epoch 130 loss=1.3196924560599421\n",
      "epoch 140 loss=1.3140851186662366\n",
      "epoch 150 loss=1.3086247050968187\n",
      "epoch 160 loss=1.303244154224797\n",
      "epoch 170 loss=1.2979256966025559\n",
      "epoch 180 loss=1.2926652607593618\n",
      "epoch 190 loss=1.2874621666226416\n",
      "epoch 200 loss=1.2823163207569455\n",
      "epoch 210 loss=1.277227558842973\n",
      "epoch 220 loss=1.2721955577663044\n",
      "epoch 230 loss=1.267219872009239\n",
      "epoch 240 loss=1.2622999799444763\n",
      "epoch 250 loss=1.2574353174610242\n",
      "epoch 260 loss=1.252625298996311\n",
      "epoch 270 loss=1.2478693298860186\n",
      "epoch 280 loss=1.2431668133866176\n",
      "epoch 290 loss=1.2385171545919895\n",
      "epoch 300 loss=1.233919762584741\n",
      "epoch 310 loss=1.2293740515974314\n",
      "epoch 320 loss=1.2248794416227093\n",
      "epoch 330 loss=1.2204353587180456\n",
      "epoch 340 loss=1.2160412351417622\n",
      "epoch 350 loss=1.2116965093961014\n",
      "epoch 360 loss=1.2074006262193424\n",
      "epoch 370 loss=1.2031530365500895\n",
      "epoch 380 loss=1.1989531974765566\n",
      "epoch 390 loss=1.1948005721779036\n",
      "epoch 400 loss=1.1906946298615684\n",
      "epoch 410 loss=1.1866348456986757\n",
      "epoch 420 loss=1.182620700758738\n",
      "epoch 430 loss=1.178651681944349\n",
      "epoch 440 loss=1.1747272819260952\n",
      "epoch 450 loss=1.1708469990780501\n",
      "epoch 460 loss=1.1670103374137755\n",
      "epoch 470 loss=1.1632168065230306\n",
      "epoch 480 loss=1.159465921509122\n",
      "epoch 490 loss=1.1557572029269285\n",
      "epoch 500 loss=1.1520901767216583\n",
      "epoch 510 loss=1.1484643741682083\n",
      "epoch 520 loss=1.144879331811282\n",
      "epoch 530 loss=1.1413345914061337\n",
      "epoch 540 loss=1.1378296998599853\n",
      "epoch 550 loss=1.134364209174094\n",
      "epoch 560 loss=1.1309376763865102\n",
      "epoch 570 loss=1.1275496635154107\n",
      "epoch 580 loss=1.12419973750313\n",
      "epoch 590 loss=1.1208874701607972\n",
      "epoch 600 loss=1.1176124381135981\n",
      "epoch 610 loss=1.1143742227466322\n",
      "epoch 620 loss=1.1111724101514178\n",
      "epoch 630 loss=1.1080065910729686\n",
      "epoch 640 loss=1.104876360857469\n",
      "epoch 650 loss=1.1017813194005597\n",
      "epoch 660 loss=1.0987210710961755\n",
      "epoch 670 loss=1.0956952247859635\n",
      "epoch 680 loss=1.092703393709307\n",
      "epoch 690 loss=1.0897451954538404\n",
      "epoch 700 loss=1.0868202519065993\n",
      "epoch 710 loss=1.0839281892056514\n",
      "epoch 720 loss=1.081068637692337\n",
      "epoch 730 loss=1.078241231863974\n",
      "epoch 740 loss=1.0754456103271575\n",
      "epoch 750 loss=1.0726814157515545\n",
      "epoch 760 loss=1.0699482948242074\n",
      "epoch 770 loss=1.0672458982043957\n",
      "epoch 780 loss=1.0645738804789469\n",
      "epoch 790 loss=1.0619319001181031\n",
      "epoch 800 loss=1.059319619431837\n",
      "epoch 810 loss=1.0567367045267\n",
      "epoch 820 loss=1.0541828252631296\n",
      "epoch 830 loss=1.0516576552132428\n",
      "epoch 840 loss=1.0491608716191112\n",
      "epoch 850 loss=1.0466921553515012\n",
      "epoch 860 loss=1.044251190869061\n",
      "epoch 870 loss=1.0418376661780049\n",
      "epoch 880 loss=1.039451272792208\n",
      "epoch 890 loss=1.0370917056937865\n",
      "epoch 900 loss=1.0347586632940888\n",
      "epoch 910 loss=1.0324518473951527\n",
      "epoch 920 loss=1.0301709631515756\n",
      "epoch 930 loss=1.0279157190328296\n",
      "epoch 940 loss=1.0256858267859854\n",
      "epoch 950 loss=1.023481001398866\n",
      "epoch 960 loss=1.0213009610636057\n",
      "epoch 970 loss=1.019145427140642\n",
      "epoch 980 loss=1.0170141241230641\n",
      "epoch 990 loss=1.0149067796014242\n",
      "\n",
      "Final parameters:\n",
      "Weight: 2.0042\n",
      "Bias: 1.0888\n",
      "\n",
      "Prediction for x=5: 11.1100\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "class LinearRegression:\n",
    "    def __init__(self, learning_rate=0.01):\n",
    "        self.bias = random.random()\n",
    "        self.weight = random.random()\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.weight * x + self.bias\n",
    "\n",
    "    def compute_loss(self, x, y_true):\n",
    "        y_pred = self.forward(x)\n",
    "        return (y_pred - y_true)**2\n",
    "\n",
    "    def backward(self,x, y_true):\n",
    "        y_pred = self.forward(x)\n",
    "        error = y_pred - y_true\n",
    "\n",
    "        dw = 2*error*x\n",
    "        db = 2*error\n",
    "\n",
    "        return dw, db\n",
    "\n",
    "    def train_step(self, x, y_true):\n",
    "        dw, db = self.backward(x, y_true)\n",
    "        self.weight = self.weight-dw*self.learning_rate\n",
    "        self.bias = self.bias-db*self.learning_rate\n",
    "\n",
    "    def train(self, X, y, epochs=1000):\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            for x_i, y_i in zip(X, y):\n",
    "                total_loss+= self.compute_loss(x_i, y_i )\n",
    "                self.train_step(x_i, y_i)\n",
    "            if epoch%10==0:\n",
    "                print(f'epoch {epoch} loss={total_loss}')\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate synthetic data\n",
    "    X = [i for i in range(10)]\n",
    "    y = [2*x + 1 + random.uniform(-0.5, 0.5) for x in X]  # y = 2x + 1 + noise\n",
    "\n",
    "    # Create and train model\n",
    "    model = LinearRegression(learning_rate=0.0001)\n",
    "    model.train(X, y, epochs=1000)\n",
    "\n",
    "    # Print final parameters\n",
    "    print(f\"\\nFinal parameters:\")\n",
    "    print(f\"Weight: {model.weight:.4f}\")\n",
    "    print(f\"Bias: {model.bias:.4f}\")\n",
    "\n",
    "    # Test predictions\n",
    "    test_x = 5\n",
    "    prediction = model.forward(test_x)\n",
    "    print(f\"\\nPrediction for x={test_x}: {prediction:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let me break down how we derive these gradient formulas step by step:\n",
    "\n",
    "First, let's write out our equations:\n",
    "\n",
    "Prediction: ŷ = wx + b\n",
    "Loss function (MSE): L = (ŷ - y)² = (wx + b - y)²\n",
    "\n",
    "\n",
    "For gradient descent, we need ∂L/∂w and ∂L/∂b. We'll use the chain rule.\n",
    "For weight (w):\n",
    "\n",
    "Using chain rule on L = (wx + b - y)²\n",
    "∂L/∂w = 2(wx + b - y) * ∂(wx + b - y)/∂w\n",
    "∂(wx + b - y)/∂w = x\n",
    "Therefore: ∂L/∂w = 2(wx + b - y)x\n",
    "In code: dw = 2 * error * x where error = (wx + b - y)\n",
    "\n",
    "\n",
    "For bias (b):\n",
    "\n",
    "Using chain rule again on L = (wx + b - y)²\n",
    "∂L/∂b = 2(wx + b - y) * ∂(wx + b - y)/∂b\n",
    "∂(wx + b - y)/∂b = 1\n",
    "Therefore: ∂L/∂b = 2(wx + b - y)\n",
    "In code: db = 2 * error where error = (wx + b - y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
