{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1078f856",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  To keep things simple, start by looking at the top 20 most viewed pages (the first 20 rows of the output generated by using .value_counts()):\n",
    "\n",
    "\n",
    "data['title'].value_counts()[:20]\n",
    "\n",
    "data['title'].value_counts()[:20].plot(kind='barh')\n",
    "\n",
    "data.sample(n=5)\n",
    "\n",
    "datasets[0].head(n=5)\n",
    "\n",
    "data = data.fillna('')\n",
    "\n",
    "#Selecting rows in a DataFrame\n",
    "data[:3]\n",
    "\n",
    "\n",
    "#Selecting a specific row\n",
    "data.iloc[1]\n",
    "\n",
    "data.iloc[:,0] # first column of data frame (first_name)\n",
    "data.iloc[:,1] # second column of data frame (last_name)\n",
    "data.iloc[:,-1] # last column of data frame (id)\n",
    "\n",
    "data.iloc[0:5] # first five rows of dataframe\n",
    "data.iloc[:, 0:2] # first two columns of data frame with all rows\n",
    "\n",
    "\n",
    "#####\n",
    "data = {\n",
    "  \"age\": [50, 40, 30, 40, 20, 10, 30],\n",
    "  \"qualified\": [True, False, False, False, False, True, True]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "newdf = df.sort_values(by='age')\n",
    "\n",
    "\n",
    "\n",
    "data.drop_duplicates(subset='k1')\n",
    "\n",
    "\n",
    "#query is the best\n",
    "\n",
    "df = pd.DataFrame({'A': range(1, 6),\n",
    "                   'B': range(10, 0, -2),\n",
    "                   'C C': range(10, 5, -1)})\n",
    "df\n",
    "   A   B  C C\n",
    "0  1  10   10\n",
    "1  2   8    9\n",
    "2  3   6    8\n",
    "3  4   4    7\n",
    "4  5   2    6\n",
    "df.query('A > B')\n",
    "\n",
    "df.query(\"`Courses Fee` >= 23000 and `Courses Fee` <= 24000\")\n",
    "\n",
    "\n",
    "#Quick Examples To Extract Column Value Based Another Column\n",
    "# Extract column values by using DataFrame.loc[] property.\n",
    "df2=df.loc[df['Fee'] == 30000, 'Courses']\n",
    "df2=df[df['Fee']==22000]['Courses']\n",
    "class_23 = titanic[titanic[\"Pclass\"].isin([2, 3])]  #VIP 这个是最靠的方法\n",
    "#another way of doing it is: convert the food values to the lower case and apply the function\n",
    "lower = lambda x: x.lower()\n",
    "data['food'] = data['food'].apply(lower)\n",
    "data['animal2'] = data.apply(meat_2_animal, axis='columns')\n",
    "data\n",
    "\n",
    "#Get all rows that contain a specific substring\n",
    "contain_values = df[df['month'].str.contains('Ju')]\n",
    "#Get all rows that contain one substring OR another substring\n",
    "contain_values = df[df['month'].str.contains('Ju|Ma')]\n",
    "\n",
    "\n",
    "\n",
    "#select rows with and without NA\n",
    "wenjuan3 = wenjuan1[wenjuan1['question_tags_true'].isnull()]\n",
    "wenjuan2 = wenjuan1.dropna(subset=['question_tags_true'])\n",
    "\n",
    "\n",
    "#####\n",
    "#The filter() function in Python takes in a function and a list as arguments. This offers an elegant way to filter out all the elements of a sequence “sequence”, for which the function returns True. Here is a small program that returns the odd numbers from an input list\n",
    "\n",
    "\n",
    "# Python code to illustrate \n",
    "# filter() with lambda() \n",
    "li = [5, 7, 22, 97, 54, 62, 77, 23, 73, 61] \n",
    "final_list = list(filter(lambda x: (x%2 != 0) , li)) \n",
    "print(final_list) \n",
    "\n",
    "Output:\n",
    "[5, 7, 97, 77, 23, 73, 61]\n",
    "\n",
    "\n",
    "\n",
    "# Use of lambda() with map()\n",
    "\n",
    "# The map() function in Python takes in a function and a list as argument. The function is called with a lambda function and a list and a new list is returned which contains all the lambda modified items returned by that function for each item. Example:\n",
    "\n",
    "# Python code to illustrate  \n",
    "# map() with lambda()  \n",
    "# to get double of a list. \n",
    "li = [5, 7, 22, 97, 54, 62, 77, 23, 73, 61] \n",
    "final_list = list(map(lambda x: x*2 , li)) \n",
    "print(final_list) \n",
    "\n",
    "\n",
    "Output:\n",
    "\n",
    "[10, 14, 44, 194, 108, 124, 154, 46, 146, 122]\n",
    "\n",
    "\n",
    "#data['title'][:3]\n",
    "\n",
    "\n",
    "medical_referrer_index = data['referrer'].str.contains('medical')\n",
    "medical_referrals = data[medical_referrer_index]\n",
    "medical_referrals\n",
    "\n",
    "\n",
    "medical_referrals['referrer'].tolist()\n",
    "\n",
    "\n",
    "\n",
    "data.sort_values(by='arr_delay', ascending=False)[:10]\n",
    "\n",
    "\n",
    "#ifelse\n",
    "\n",
    "pp2[\"switch_to_unfavored\"] = np.where(pp2['choicevalue'] == pp2[\"favored\"], 0, 1)\n",
    "\n",
    "\n",
    "rawdata[\"format1\"] = np.where(rawdata[\"investment_amount\"].isnull(), \"ask_amount\", \"other\")\n",
    "rawdata[\"format1\"]\n",
    "\n",
    "    \n",
    "\n",
    "# The technique you learned in the previous lesson calls for you to create a function, then use the .apply() method like this:\n",
    "def is_delayed(x):\n",
    "    return x > 0\n",
    "data['delayed'] = data['arr_delay'].apply(is_delayed)\n",
    "\n",
    "data['delayed'] = data['arr_delay'].apply(lambda x: x > 0)\n",
    "\n",
    "\n",
    "\n",
    "#another way of doing it is: convert the food values to the lower case and apply the function\n",
    "lower = lambda x: x.lower()\n",
    "data['food'] = data['food'].apply(lower)\n",
    "data['animal2'] = data.apply(meat_2_animal, axis='columns')\n",
    "data\n",
    "\n",
    "\n",
    "#####\n",
    "#calculate means of each group\n",
    "data.pivot_table(values='ounces',index='group',aggfunc=np.mean)\n",
    "group\n",
    "a    6.333333\n",
    "b    7.166667\n",
    "c    4.666667\n",
    "Name: ounces, dtype: float64\n",
    "\n",
    "\n",
    "#calculate count by each group\n",
    "data.pivot_table(values='ounces',index='group',aggfunc='count')\n",
    "group\n",
    "a    3\n",
    "b    3\n",
    "c    3\n",
    "Name: ounces, dtype: int64\n",
    "\n",
    "####\n",
    "dictionary = df.to_dict()\n",
    "df.describe()\n",
    "df.info()\n",
    "\n",
    "df = df.drop(['et_idx','et_code','et_desc','et_unit'], axis=1)\n",
    "\n",
    "#####\n",
    "# Recoding all missing data\n",
    "df.fillna(0, inplace=True)\n",
    "# Recoding missing data in a special column\n",
    "s = df['AVERAG'].fillna(0)\n",
    "# Working with strings\n",
    "s = df['col'].str.lower()\n",
    "s = df['col'].str.upper()\n",
    "s = df['col'].str.len()\n",
    "s = df['col'].str.replace('old', 'new')\n",
    "\n",
    "\n",
    "######\n",
    "group_by_carrier = data.groupby(['unique_carrier','delayed'])\n",
    "\n",
    "# hink of groupby() as splitting the dataset data into buckets by carrier (‘unique_carrier’), and then splitting the records inside each carrier bucket into delayed or not delayed (‘delayed’). The result is assigned to the group_by_carrier variable.\n",
    "\n",
    "# What happens next gets tricky. If you just look at the group_by_carrier variable, you'll see that it is a DataFrameGroupBy object. You can think of that as instructions on how to group, but without instructions on how to display values:\n",
    "\n",
    "<pandas.core.groupby.DataFrameGroupBy object at 0x10c6df610>\n",
    "\n",
    "\n",
    "# You need to provide instructions on what values to display. For example:\n",
    "\n",
    "# You're grouping all of the rows that share the same carrier, as well as all the rows that share the same value for delayed. You need to tell the function what to do with the other values. You could do any number of things:\n",
    "\n",
    "# average the number of minutes of delay\n",
    "# sum the total number of minutes of delay\n",
    "\n",
    "group_by_carrier.size()\n",
    "\n",
    "# That's exactly what you're looking for! It's a little hard to read, though. Pandas has a handy .unstack() method—use it to convert the results into a more readable format and store that as a new variable, count_delays_by_carrier\n",
    "\n",
    "count_delays_by_carrier = group_by_carrier.size().unstack()\n",
    "count_delays_by_carrier\n",
    "\n",
    "\n",
    "count_delays_by_carrier.plot(kind='barh', stacked=True, figsize=[16,6], colormap='winter')\n",
    "\n",
    "\n",
    "\n",
    "flights_by_carrier = data.pivot_table(index='flight_date', columns='unique_carrier', values='flight_num', aggfunc='count')\n",
    "flights_by_carrier.head()\n",
    "\n",
    "\n",
    "#you can pivot on the date and type of delay, delays_list, summing the number of minutes of each type of delay:\n",
    "delays_list = ['carrier_delay','weather_delay','late_aircraft_delay','nas_delay','security_delay']\n",
    "flight_delays_by_day = data.pivot_table(index='flight_date', values=delays_list, aggfunc='sum')\n",
    "flight_delays_by_day\n",
    "\n",
    "\n",
    "\n",
    "##### sort\n",
    "aa= [[4, 2.0], [1, 1.0], [3, 0.8], [2, 0.25]]\n",
    "\n",
    "aa.sort(key=lambda x: (-x[1], x[0]))\n",
    "\n",
    "\n",
    "#可以灵活调整输出变量\n",
    "DF_list2[0].groupby(['year','month']).agg(\n",
    "             like=('likecount', 'mean'),\n",
    "             clicks=('clicksCount', 'mean'))\n",
    "             \n",
    "\n",
    "\n",
    "data.groupby('Sex').agg({'PassengerId': 'count'})\n",
    "\n",
    "\n",
    "#############\n",
    "y_train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "\n",
    ">>> a = numpy.array([1, 2, 1.8E308, 1.8E308, 42])\n",
    ">>> a[a < 1E308] # use whatever threshold you like\n",
    "array([  1.,   2.,  42.])\n",
    "\n",
    "aaa = (y_pred - np.array(y_test))\n",
    "aaa = aaa[~np.isnan(aaa)]\n",
    "aaa = aaa[aaa < 1E308]\n",
    "aaa = aaa[aaa > -1E308]          \n",
    "\n",
    "\n",
    "######\n",
    "pd.merge(frame_1, frame_2, left_on='county_ID', right_on='countyid'， how = 'left')\n",
    "\n",
    "\n",
    "#reorder pandas columns\n",
    "questions[\"cluster\"] = cluster_assignment\n",
    "questions.insert(2, 'cluster', questions.pop('cluster'))\n",
    "\n",
    "\n",
    "###这里的names是一个List\n",
    "import pandas\n",
    "df = pandas.DataFrame(data=names)\n",
    "df.to_csv(\"./file.csv\", sep=',',index=False)\n",
    "\n",
    " # alwasy include this index = False to avoid new column\n",
    " \n",
    " \n",
    " ####### create pandas dataframe\n",
    " import pandas as pd\n",
    "lst1 = range(100)\n",
    "lst2 = range(100)\n",
    "lst3 = range(100)\n",
    "percentile_list = pd.DataFrame(\n",
    "    {'lst1Title': lst1,\n",
    "     'lst2Title': lst2,\n",
    "     'lst3Title': lst3\n",
    "    })\n",
    "    \n",
    "    \n",
    "\n",
    "#Using get_dummies approach\n",
    "one_hot_encoded_data = pd.get_dummies(data, columns = ['Remarks', 'Gender'])\n",
    "print(one_hot_encoded_data)\n",
    "\n",
    "\n",
    "###############\n",
    "data = pd.read_csv('Employee_data.csv')\n",
    "  \n",
    "# Converting type of columns to category\n",
    "data['Gender']=data['Gender'].astype('category')\n",
    "data['Remarks']=data['Remarks'].astype('category')\n",
    "  \n",
    "  \n",
    "#Assigning numerical values and storing it in another columns\n",
    "data['Gen_new']=data['Gender'].cat.codes\n",
    "data['Rem_new']=data['Remarks'].cat.codes \n",
    "  \n",
    "  \n",
    "#Create an instance of One-hot-encoder\n",
    "enc=OneHotEncoder()\n",
    "  \n",
    "#Passing encoded columns\n",
    "'''\n",
    "NOTE: we have converted the enc.fit_transform() method to array because the fit_transform method \n",
    "of OneHotEncoder returns SpiPy sparse matrix this enables us to save space when we \n",
    "have huge  number of categorical variables\n",
    "'''\n",
    "enc_data=pd.DataFrame(enc.fit_transform(data[['Gen_new','Rem_new']]).toarray())\n",
    "  \n",
    "#Merge with main\n",
    "New_df=data.join(enc_data)\n",
    "\n",
    "\n",
    "    \n",
    " \n",
    "###### 这个办法更省心，但不能指定code 的 matching relationship\n",
    "le = LabelEncoder()\n",
    "full_data_w_features[\"type_of_issuer_label\"] = le.fit_transform(full_data_w_features[\"type_of_issuer\"])\n",
    "full_data_w_features[\"currency_label\"] = le.fit_transform(full_data_w_features[\"currency\"])\n",
    "full_data_w_features[\"security_type_label\"] = le.fit_transform(full_data_w_features[\"security_type\"])\n",
    "full_data_w_features[\"email_domain_label\"] = le.fit_transform(full_data_w_features[\"email_domain\"])\n",
    " \n",
    " \n",
    " ######\n",
    " Next, there are both categorical and numeric features. We will build two separate pipelines and combine them later.\n",
    "\n",
    "The next code examples will heavily use Sklearn-Pipelines. If you are not familiar with them, check out my separate article for the complete guide on them.\n",
    "\n",
    "For the categorical features, we will impute the missing values with the mode of the column and encode them with One-Hot encoding:\n",
    "\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "categorical_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"impute\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"oh-encode\", OneHotEncoder(handle_unknown=\"ignore\", sparse=False)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "For the numeric features, I will choose the mean as an imputer and StandardScaler so that the features have 0 mean and a variance of 1:\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "numeric_pipeline = Pipeline(\n",
    "    steps=[(\"impute\", SimpleImputer(strategy=\"mean\")), \n",
    "           (\"scale\", StandardScaler())]\n",
    ")\n",
    "\n",
    "Finally, we will combine the two pipelines with a column transformer. To specify which columns the pipelines are designed for, we should first isolate the categorical and numeric feature names:\n",
    "\n",
    "\n",
    "cat_cols = X.select_dtypes(exclude=\"number\").columns\n",
    "num_cols = X.select_dtypes(include=\"number\").columns\n",
    "\n",
    "\n",
    "Next, we will input these along with their corresponding pipelines into a ColumnTransFormer instance:\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "full_processor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"numeric\", numeric_pipeline, num_cols),\n",
    "        (\"categorical\", categorical_pipeline, cat_cols),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Apply preprocessing\n",
    "X_processed = full_processor.fit_transform(X)\n",
    "y_processed = SimpleImputer(strategy=\"most_frequent\").fit_transform(\n",
    "    y.values.reshape(-1, 1)\n",
    ")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_processed, y_processed, stratify=y_processed, random_state=1121218\n",
    ")\n",
    "\n",
    "def process_security_type(data):\n",
    "    security_types = [x.lower() for x in data['security_type'].unique()]\n",
    "    security_mapping = {}\n",
    "    for x in security_types:\n",
    "        if 'warrant' in x:\n",
    "            security_mapping[x] = 'Derivatives'\n",
    "        elif 'share' in x:\n",
    "            security_mapping[x] = 'Ownership'\n",
    "        elif 'convertible' in x:\n",
    "            security_mapping[x] = 'Ownership'\n",
    "        elif 'unit' in x:\n",
    "            security_mapping[x] = 'Ownership'\n",
    "        elif 'bond' in x:\n",
    "            security_mapping[x] = 'Bonds'\n",
    "        elif 'debenture' in x:\n",
    "            security_mapping[x] = 'Bonds'\n",
    "        elif 'note' in x:\n",
    "            security_mapping[x] = 'Bonds'\n",
    "        else:\n",
    "            security_mapping[x] = 'Ownership'\n",
    "    return data.assign(security_type_processed=data['security_type']\n",
    "                       .map(lambda x: security_mapping[x.lower()]))\n",
    "                       \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
